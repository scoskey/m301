\documentclass[11pt,oneside]{amsbook}

\title{Introduction to linear algebra}
\author{Course notes written by Samuel Coskey\\Based on material from ``Introduction to linear algebra'' by Gilbert Strang}

\usepackage[vscale=.8,vmarginratio=4:3]{geometry}
\usepackage{mathpazo,amssymb}
\usepackage{setspace}\onehalfspacing\raggedbottom
\renewcommand{\labelitemi}{$\circ$}
\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\chaptername}{Part}
\renewcommand{\thechapter}{\Roman{chapter}}
\usepackage{remreset}
\makeatletter\@removefromreset{section}{chapter}\makeatother
\usepackage{etoolbox}
\makeatletter
\pretocmd{\@seccntformat}{\S}{}{}
\patchcmd{\tocsection}{#2.}{\S#2.}{}{}
\apptocmd{\tocsection}{\dotfill}{}{}
\patchcmd{\@maketitle}{\newpage}{}{}{}
\makeatother
\usepackage[linktoc=all,colorlinks,linkcolor=blue]{hyperref}
\usepackage{systeme}
\usepackage{tikz}

\newcommand{\set}[1]{\left\{\,#1\,\right\}}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\rng}{rng}
\DeclareMathOperator{\col}{col}

\theoremstyle{definition}
\newtheorem{exerc}{Exercise}[section]
\swapnumbers
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem*{notes}{Notes and further reading}
\newtheorem*{reading}{Reading}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\renewcommand{\theequation}{\arabic{section}.e\arabic{equation}}
\renewcommand{\thefigure}{\arabic{section}.f\arabic{figure}}
\newcounter{activityitem}
\newenvironment{activity}{\begin{list}{\arabic{activityitem}.}{\usecounter{activityitem}\setlength{\itemsep}{.2in}}}{\end{list}}

\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Systems of linear equations, vectors, and matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systems of linear equations and vectors}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{reading}
  Strang, ILA 5th, \S 2.1.
\end{reading}

What is the equation of a line? The most commonly taught version is $y=mx+b$, and there are many other correct answers to the question. However the \emph{standard form} of a line is $ax+by=c$. This is considered standard because it treats the two variables symmetrically. In this course we will look at generalizations of this simple type of equation.

\begin{itemize}
  \item We will consider equations with more than two variables $x,y$. For example if we want to have three variables $x,y,z$, then the analogous equation is $ax+by+cz=d$. Of course, this is not the equation of a line (what is it?). But due to its similarity we still call it a \emph{linear equation}.
  \item We will consider situations when we have more than one equation. For example we can have the two equations true simultaneously: $\{ax+by=c;\ dx+ey=f\}$. We call this a \emph{system of linear equations}.
\end{itemize}

Let's look at a few examples of simple problems which can be modeled by a system of linear equations.

\begin{example}[Economics]
  The New York Times reported that the film ``The Interview'' had two million online sales, split between rentals and downloads. A rental costs \$6 and a download costs \$15. The total revenue was \$15 million. The NYT could not determine how many transactions were rentals and how many were sales. Can we help them? The information given can be expressed in a system of two linear equations.
  \[\systeme{r+d=2\text{ million},6r+15d=15\text{ million}}
  \]
\end{example}

\begin{example}[Modeling flow]
  Vehicles driving in each segment of a one-way grid are counted during a period of one hour. The results are shown in the figure.
  \begin{center}
    \begin{tikzpicture}[shorten <=2pt,shorten >=2pt,scale=1.5]
      \draw[->] (0,1) -- node[fill=white]{$520$} (1,1);
      \draw[->] (1,1) -- node[fill=white]{$w$} (2,1);
      \draw[->] (2,1) -- node[fill=white]{$600$} (3,1);

      \draw[<-] (0,2) -- node[fill=white]{$610$} (1,2);
      \draw[<-] (1,2) -- node[fill=white]{$y$} (2,2);
      \draw[<-] (2,2) -- node[fill=white]{$640$} (3,2);

      \draw[<-] (1,0) -- node[fill=white]{$480$} (1,1);
      \draw[<-] (1,1) -- node[fill=white]{$z$} (1,2);
      \draw[<-] (1,2) -- node[fill=white]{$450$} (1,3);

      \draw[->] (2,0) -- node[fill=white]{$390$} (2,1);
      \draw[->] (2,1) -- node[fill=white]{$x$} (2,2);
      \draw[->] (2,2) -- node[fill=white]{$310$} (2,3);
    \end{tikzpicture}
  \end{center}
  Assume that for each intersection, the number of cars entering it is the same as the number of cars leaving it. What are the values of the unknowns? Again with a little work we can approach this using a system of linear equations.
  \[\begin{cases}x+640=y+310\\y+450=z+610\\z+520=w+480\\w+390=x+600\end{cases}
  \]
\end{example}

\begin{example}[Best fit line]
  Given sampled data points, what is the equation of a trend line that best describes the overall behavior of the data? For example suppose you are measuring the change in a quantity over time. At time $t=2$ you measure $3$, at time $t=3$ you measure $5$, and at time $t=6$ you measure $6$. We want to find the coefficients $m,b$ of a line $Y=mX+b$ that best fits the data.
  
  \begin{center}
    \begin{tikzpicture}[scale=.7]
      \draw[gray!30,very thin,dashed] (-0.9,-0.9) grid (6.9,6.9);
      \draw[->,thick] (-1,0)--(7,0) node[right] {$X$};
      \draw[->,thick] (0,-1)--(0,7) node[left] {$Y$};
      \draw[fill] (2,3) circle (.1);
      \draw[fill] (3,5) circle (.1);
      \draw[fill] (6,6) circle (.1);
      \draw[domain=-.9:6.9,dashed] plot (\x,.6538*\x+2.269) node[right]{???};
    \end{tikzpicture}
  \end{center}

  The line misses the data points by $2m+b-3$, $3m+b-5$, and $6m+b-6$. To minimize these three error terms, we usually minimize the sum of their squares: $E=(2m+b-3)^2+(3m+b-5)^2+(6m+b-6)^2$. Thus we set $\partial E/\partial m$ and $\partial E/\partial b$ to zero. With a little algebra, this amounts to solving the system:
  \[\systeme{49m+11b-57=0,11m+3b-14=0}
  \]
\end{example}
  
Our short term goal will be to learn how to solve systems like these. But before we begin doing so, let's visualize systems and their solutions geometrically. Consider the following example system:
\[\systeme{x-2y=1,3x+2y=11}
\]

The classical way to visualize the system and its solution is to graph the two equations and highlight the point or points where they intersect.
\begin{center}
  \begin{tikzpicture}
    \draw[gray!30,very thin,dashed] (-0.9,-0.9) grid (4.9,4.9);
    \draw[->,thick] (-1,0)--(5,0);
    \draw[->,thick] (0,-1)--(0,5);
    \draw[domain=0:4] plot (\x,.5*\x-.5) node[right]{$x-2y=1$};
    \draw[domain=1:4] plot (\x,11/2-3/2*\x);
    \node[left] at (1,4) {$3x+2y=11$};
  \end{tikzpicture}
\end{center}

But since the system is linear, there is another very revealing way to visualize the system and its solutions. We will consider the system as a \emph{single equation} with \emph{vector coefficients}.
\[\begin{bmatrix}1\\3\end{bmatrix}x+\begin{bmatrix}-2\\2\end{bmatrix}y
  =\begin{bmatrix}1\\11\end{bmatrix}
\]

We should say a little bit about vectors. First, if you are coming from calculus, you may be used to writing vectors horizontally. Now we write them vertically in order to fit our two equations together. Next, recall that we can scale vectors by lenghening and shortening them by a scalar factor, and that we can add vectors ``tail to tip''.

Viewed this way, the system is asking, how many of the vector $(1,3)$ and how many of the vector $(-2,2)$ do we need in order to obtain the vector $(1,11)$?

\begin{center}
  \begin{tikzpicture}[scale=.5]
    \draw[gray!30,very thin,dashed] (-2.9,-0.9) grid (3.9,12.9);
    \node[circle,fill,draw,inner sep=1.5,label=right:{\small$\begin{bmatrix}1\\11\end{bmatrix}$}] at (1,11) (tip) {};
    \draw[->,thick] (-3,0)--(4,0);
    \draw[->,thick] (0,-1)--(0,12);
    \draw[->] (0,0)--(1,3) node[right]{\small$\begin{bmatrix}1\\3\end{bmatrix}$};
    \draw[dashed,->] (0,0)--(3,9) node[right]{\small$3\cdot\begin{bmatrix}1\\3\end{bmatrix}$};
    \draw[dashed,->] (3,9)--(tip);
    \draw[->] (0,0)--(-2,2) node[left]{\small$\begin{bmatrix}-2\\2\end{bmatrix}$};
    \draw[dashed,->] (-2,2)--(tip);
  \end{tikzpicture}
\end{center}

In both the classical and vector views, the solution is $x=3$, $y=1$.

% Note that geogebra can be extremely helpful to visualize the row picture! It also shows the effect of elimination on the system.

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Solve the three systems of linear equations from the first lecture using any method.
  \begin{enumerate}
    \item (costs) \systeme{r+d=2\text{ million},6r+15d=15\text{ million}}
    \item (traffic flow) $\begin{cases}x+640=y+310\\y+450=z+610\\z+520=w+480\\w+390=x+600\end{cases}$
    \item (least squares) \systeme{49m+11b-57=0,11m+3b-14=0}
  \end{enumerate}
  \item For each of the following systems of linear equations, draw the ``row picture'' (classical) and the ``column picture'' (vectors). Then find the solution using any method.
  \begin{enumerate}\itemsep 5pt
    \item \systeme{3x+2y=6,2x+3y=6}
    \item \systeme{x-2y=0,x+y=6}
  \end{enumerate}
  \item In your own words, describe a reasonable method for solving systems of linear equations such as the ones on this page.
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Elimination}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 2.2.

In the previous class we argued using three examples that it is \emph{important} to solve systems of linear equations. In this section we will address \emph{how} to solve systems of linear equations. Of course, there are dozens of valid methods in existence. For example, some students use \emph{substitution}, that is, solve for one variable in one equation, and then plug the result into the second equation.

In this class we will provide you with an efficient, deterministic method for solving systems of linear equations, called \emph{elimination}. You will be expected to be able to solve systems using this method, even if you are aware of others. The elimination method has a lot of key benefits---it is efficient and it reveals valuable information about the system. Moreover, we all agree (and our computers agree) to use the same exact steps, we can do linear algebra harmoniously together.

We will now illustrate the method with the simple example from the last class.

\begin{example}
  Consider the system
  \[\systeme{x-2y=1,3x+2y=11}
  \]
  We begin by ``eliminating'' the $3x$ term. To do this we take the first equation minus three times the second equation, and replace the original second equation with the result. We notate this operation or step by $R_2\leftarrow R_2-3R_1$.
  \[\systeme{x-2y=1,8y=8}
  \]
  The elimination is now done in one step. The system is \emph{triangular}, meaning that the only nonzero coefficients are on or above the diagonal. Moreover the new system is equivalent to the old system, in the sense that they have the same solutions.

  Having finished elimination, the system can now be easily solved by \emph{back-substitution}. Here we use the last equation to solve for the last variable, the second-to-last to solve for the second-to-last, and so on if there are more equations. The solution is $x=3$, $y=1$.
\end{example}

In the previous example, there was only one step of elimination. For larger systems there will be several steps.

\begin{example}
  Consider the system
  \[\systeme{2x+4y-2z=2,4x+9y-3z=8,-2x-3y+7z=10}
  \]
  Begin by using the upper-left term $2x$ as a \emph{pivot} to eliminate the two terms below it. To do this, we will perform the two operations $R_2\leftarrow R_2-2R_1$ and $R_3\leftarrow R_3+1R_1$.
  \[\systeme{2x+4y-2z=2,y+z=4,y+5z=12}
  \]
  Next we move to the second column and use the $1y$ (in the second row) as a pivot to eliminate the term below it. This time we perform the operation $R_3\leftarrow R_3-1R_2$.
  \[\systeme{2x+4y-2z=2,y+z=4,4z=8}
  \]
  The system is now upper triangular, and we can back-solve starting with the last equation and the last variable. The solution is $-1,2,2$.
\end{example}

There is one important case when the elimination steps described above cannot be used. This occurs when the term we want to use as a pivot is $0$. To get around this issue, we exchange the order of the equations.

\begin{example}
  In the following system, the first equation has $0x$ in its pivot location, so we will need to exchange it with the second row. We denote this operation $R_1\leftrightarrow R_2$.
  \[\systeme{5y-7z=2,2x+y+z=1,x-y+z=3}
  \]
\end{example}

\begin{example}
  In the following system, there are not as many pivots as there are variables. In this case we can still perform elimination, but we cannot back-solve to get a unique solution. In fact the system has infinitely many solutions.
  \[\systeme{x-y+2z=1,2x+y-z=-1,5x-2y+5z=2}
  \]
\end{example}

\begin{example}
  In the following system, there are once again not as many pivots as there are variables. In this case the system has no solution.
  \[\systeme{x-y+2z=1,2x-2y+z=3,-x+y-z=-1}
  \]
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Use the elimination method to reduce each system to an upper triangular one. For each step of the algorithm, write the notation for the step you are performing (\emph{e.g.}, $R_2\leftarrow R_2+3R_1$). Then, use back-solving to find a unique solution, or to show the system has infinitely many solutions or no solutions.
  \begin{enumerate}
    % next time: pick ones with a solution first!!
    \item \systeme{2x-3y=5,-4x+6y=8}\vspace{.2in}
    \item \systeme{3x-4y+z=-7,6x-8y-z=-16,x+2y+2z=6}
  \end{enumerate}
  \item Consider the following system of equations with unknown coefficients $a$, $b$, $c$, and $d$.
  \[\systeme[xyz]{2x+y+3z=a,5y+7z=b,dz=c}
  \]
  \begin{enumerate}
  \item Choose values of $a,b,c,d$ so that there is no solution. Explain why there is no solution.
  \item Choose values of $a,b,c,d$ so that there are infinitely many solutions. Explain why there are infinitely many solutions.
  \end{enumerate}
  % \item Look at the matrix $A$ and the vector $\mathbf{b}$. Is it possible to combine the columns of $A$, using scalar multiples and addition, to get $b$ as a result?
  % \[A=\begin{bmatrix}1&2\\-1&-1\end{bmatrix},\qquad \mathbf{b}=\begin{bmatrix}1\\2\end{bmatrix}
  % \]
  \item For each system from \S1, \#2, perform one step of elimination to get the systems below. Plot these new ``row pictures'' and ``column pictures''. What are the relationships between the old pictures and the new ones?
  \begin{enumerate}\itemsep6pt
    \item \systeme{3x+2y=6,\frac{5}{3}y=2}
    \item \systeme{x-2y=0,3y=6}
  \end{enumerate}
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix operations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 2.4.

When writing systems of linear equations, we rewrite the same variable over and over again. Starting in the next class we will simplify our notation by writing the coefficients as a matrix and the variables as a vector.

However before we dive headlong into new notation, we need to recall the concepts of matrix, vector, and the matrix-times-vector operation. For example, consider the following system.
\[\systeme{3x-2y+z=4,2x+2y-z=0,-x+y+z=1}
\]
The matrix of coefficients of the left-hand-side is
\[\begin{bmatrix}3&-2&1\\2&2&-1\\-1&1&1\end{bmatrix}
\]
The vector of variables is
\[\begin{bmatrix}x\\y\\z\end{bmatrix}
\]
The vector of scalars on the right-hand-side is
\[\begin{bmatrix}4\\0\\1\end{bmatrix}
\]
This system can be written in matrix vector form as follows:
\[\begin{bmatrix}3&-2&1\\2&2&-1\\-1&1&1\end{bmatrix}
  \begin{bmatrix}x\\y\\z\end{bmatrix}
  =\begin{bmatrix}4\\0\\1\end{bmatrix}
\]

For this last expression to make sense, we must define what it means to multiply a matrix by a vector. Here if $A$ is a matrix and $\mathbf{v}$ is a vector (where the number of columns of $A$ is equal to the number of entries of $\mathbf{v}$), then writing $A\mathbf{v}$ means we multiply each row of $A$ by  $\mathbf{v}$ in the dot product fashion to obtain a new vector $A\mathbf{v}$. For instance
\[\begin{bmatrix}2&3\\-1&2\end{bmatrix}
  \begin{bmatrix}5\\7\end{bmatrix}
  =\begin{bmatrix}(2)(5)+(3)(7)\\(-1)(5)+(2)(7)\end{bmatrix}
  =\begin{bmatrix}31\\9\end{bmatrix}
\]
We will call this the ``matrix-times-vector'' operation.

Matrices are more than just a convenient way to represent the coefficients of a system of linear equations, and they will be an important feature throughout this course. For example, recalling that vectors may be scaled or added together, matrices may be scaled or added together too. The operations are performed component by component. For example:
\[3\begin{bmatrix}2&1\\3&4\end{bmatrix}
  =\begin{bmatrix}3(2)&3(1)\\3(3)&3(4)\end{bmatrix}
  =\begin{bmatrix}6&3\\9&12\end{bmatrix}
\]
\[\begin{bmatrix}2&1\\3&4\end{bmatrix}
  +\begin{bmatrix}1&-1\\-2&5\end{bmatrix}
  =\begin{bmatrix}2+1&1+(-1)\\3+(-2)&4+5\end{bmatrix}
  =\begin{bmatrix}3&0\\1&9\end{bmatrix}
\]

Unlike vectors, two matrices of the same size may be multiplied together in a meaningful way. Here each row of the first matrix is multiplied by each column of the second matrix, sum-of-products style.
\[\begin{bmatrix}2&3\\1&5\end{bmatrix}
  \begin{bmatrix}1&4\\2&-1\end{bmatrix}
  =\begin{bmatrix}(2)(1)+(3)(2)&(2)(4)+(3)(-1)\\(1)(1)+(5)(2)&(1)(4)+(5)(-1)\end{bmatrix}
  =\begin{bmatrix}8&5\\11&-1\end{bmatrix}
\]

Matrices do not need to be square. We say a matrix is $a\times b$ if it has $a$ rows and $b$ columns. (We \emph{always} write rows $\times$ columns, not the other way around!) When multiplying matrices, the dot products have to match in length. This means the number of columns of the first matrix must match the number of rows of the second matrix.
\begin{quotation}
  \textbf{The rule.} if $A$ is $a\times b$ and $B$ is $c\times d$ then to multiply $AB$ we must have $b=c$, and the result will be $a\times d$.
\end{quotation}
Here is an example.
\[\begin{bmatrix}2&3&-1\\1&1&0\end{bmatrix}
  \begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}
  =\begin{bmatrix}6&10\\4&6\end{bmatrix}
\]

The additon and multiplication operations give us a new kind of \emph{matrix algebra}. Matrix algebra even has an analog of $0$ and $1$. The analog of $0$ is called the \emph{zero matrix} because every entry is zero:
\[\begin{bmatrix}1&2&3\\4&5&6\end{bmatrix}
  +\begin{bmatrix}0&0&0\\0&0&0\end{bmatrix}
  =\begin{bmatrix}1&2&3\\4&5&6\end{bmatrix}
\]
For square matrices, the analog of $1$ is called the \emph{identity matrix}. The identity matrix isn't filled with ones. Instead it has ones down the diagonal:
\[\begin{bmatrix}1&2\\3&4\end{bmatrix}
  \begin{bmatrix}1&0\\0&1\end{bmatrix}
  =\begin{bmatrix}1&2\\3&4\end{bmatrix}
\]
It is natural to ask whether matrix algebra obeys the same laws as traditional algebra: associativity, commutativity, and distributivity.

\newpage
\subsection*{Activity for \S \thesection}

\noindent The following problems refer to the $2\times2$ matrices:
\[A=\begin{bmatrix}2&-3\\-1&1\end{bmatrix},\qquad
  B=\begin{bmatrix}0&6\\2&2\end{bmatrix},\qquad
  C=\begin{bmatrix}1&2\\3&4\end{bmatrix}
\]

\begin{activity}
  \item Test whether the usual laws of arithmetic apply to these three matrices.
  \begin{itemize}
    \item Associativity: Calculate and compare $(AB)C$ and $A(BC)$
    \item Commutativity: Calculate and compare $AB$ and $BA$, and then again for $BC$ and $CB$
    \item Distributivity: Calculate and compare $A(B+C)$ and $AB+AC$
    \item Another rule: Calculate and compare $(B+C)^2$ and $B^2+2BC+C^2$
  \end{itemize}
  \item Find your own example of $2\times2$ matrices $A,B$ with the property that: $A,B$ are not the same, not zero, not the identity matrix, and $AB=BA$.
  \item Let $A$ be the matrix below. Find $A^2$ and $A^3$. What will $A^{100}$ be and why?
  \[\begin{bmatrix}1&0&0\\0&1&0\\0&a&1\end{bmatrix}
  \]
  
\end{activity}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Elimination using matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 2.3.

We have previously seen that a system of linear equations can be viewed as a single matrix equation $A\mathbf{x}=\mathbf{b}$. For example, consider the following system
\[\systeme{2x+4y-2z=2,4x+9y-3z=8,-2x-3y+7z=10}
\]
We can rewrite it as
\[\begin{bmatrix}2&4&-2\\4&9&-3\\-2&-3&7\end{bmatrix}
  \begin{bmatrix}x\\y\\z\end{bmatrix}
  =\begin{bmatrix}2\\8\\10\end{bmatrix}
\]

In this section we will see that row elimination corresponds to multiplying both sides of the matrix equation by a special matrix.

\begin{example}
  Continuing the example from above, the first step of elimination will be $R_2\leftarrow R_2-2R_1$. We can carry this out by multiplying both sides (on the left) by an ``elimination matrix'':
  \[\begin{bmatrix}1&&\\-2&1&\\&&1\end{bmatrix}
    \begin{bmatrix}2&4&-2\\4&9&-3\\-2&-3&7\end{bmatrix}
    \begin{bmatrix}x\\y\\z\end{bmatrix}
    =\begin{bmatrix}1&&\\-2&1&\\&&1\end{bmatrix}
    \begin{bmatrix}2\\8\\10\end{bmatrix}
  \]
  Resulting in:
  \[\begin{bmatrix}2&4&-2\\0&1&1\\-2&-3&7\end{bmatrix}
    \begin{bmatrix}x\\y\\z\end{bmatrix}
    =\begin{bmatrix}2\\4\\10\end{bmatrix}
  \]
  Thus carrying out $R_2\leftarrow R_2-2R_1$.
\end{example}

Every step of row elimination has a corresponding elimination matrix. Here are the rules
\begin{itemize}
  \item The step $R_i\leftarrow R_i+\ell R_j$ has the matrix with $1$'s down the diagonal and an $\ell$ in position $(i,j)$.
  \item The step $R_i\leftrightarrow R_j$ has a matrix with $1$'s down the diagonal except row $i$ has a $1$ in position $j$ and row $j$ has a $1$ in position $i$.
  \item The (so far unused) step $R_i\leftarrow kR_i$ has a matrix with $1$'s down the diagonal except for a $k$ in position $(i,i)$.
\end{itemize}
This means elimination can be viewed as successive multiplication by elimination matrices:
\[E_k\cdots E_2E_1A\mathbf{x}=E_k\cdots E_2E_1\mathbf{b}
\]
Until the left hand side is in its echelon (or upper triangular) form:
\[U\mathbf{x}=\mathbf{b}_{\text{new}}
\]

\begin{example}
  Continuing the previous example, we next apply the elimination matrices
  \[E_2=\begin{bmatrix}1\\&1\\1&&1\end{bmatrix},\ 
    E_3=\begin{bmatrix}1\\&1\\&-1&1\end{bmatrix}
  \]
\end{example}

Note that since we do the same operation to the left and right sides, we often combine the left- and right-hand sides into a combined \emph{augmented matrix}. This is the final and most compact way to write a system of linear equations. 

\begin{example}
  Consider the system:
  \[\systeme{x-2y+2z=1,-3x+y+z=-1,-2x+2y+z=1}
  \]
  The augmented form of the system is:
  \[\begin{bmatrix}
      1&-2&2&\vline&1\\
      -3&1&1&\vline&-1\\
      -2&2&1&\vline&1
    \end{bmatrix}
  \]
  To solve this we can apply the following elimination matrices.
  \[E_1=\begin{bmatrix}1\\3&1\\&&1\end{bmatrix},\ 
    E_2=\begin{bmatrix}1\\&1\\2&&1\end{bmatrix},\ 
    E_3=\begin{bmatrix}1\\&1\\&-\frac25&1\end{bmatrix}
  \]
\end{example}

Since the augmented matrix hides the variables, it is easy to forget that it represents a system of linear equations. If you are confused, try labeling each column with the corresponding variable name!

So far we have only used the first type of elimination step, and the first type of elimination matrix. The following example has a row exchange.

\begin{example}
  Consider the following augmented matrix.
  \[\begin{bmatrix}2&3&1&\vline&3\\2&3&2&\vline&5\\4&1&3&\vline&3
    \end{bmatrix}
  \]
  To solve this we can apply the following elemination matrices.
  \[E_1=\begin{bmatrix}1\\-1&1\\&&1\end{bmatrix},\ 
    E_2=\begin{bmatrix}1\\&1\\-2&&1\end{bmatrix},\ 
    E_3=\begin{bmatrix}1\\&&1\\&1\end{bmatrix}
  \]
\end{example}


\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Write down the $4\times4$ matrices corresponding to the row operations:
    \begin{enumerate}
    \item $R_2\leftarrow R_2-5R_1$
    \item $R_4\leftarrow R_4+7R_2$
    \item $R_2\leftrightarrow R_4$
    \end{enumerate}
  \item For each system of equations, write the augmented matrix version of the system and use elimination to solve it. In each step, write the elimination matrix you used.
    \begin{enumerate}
    \item \systeme{2x-3y=5,x-y=2}\vspace{.3in}
    \item \systeme{v+2w=0,u-v-w=-1,2u+v=4}
    \end{enumerate}
  \item Consider the $3\times3$ row operations $R_2\leftarrow R_2-2R_1$ (op1) and $R_3\leftarrow R_3+R_1$ (op2).
  \begin{itemize}
    \item Write down an elimination matrix for op1 and another one for op2.
    \item Write down an elimination matrix which undoes op1 and another one which undoes op2.
    \item Write down an elimination matrix which does op1 and op2 at the same time.
    \item Consider the operation $R_3\leftarrow R_3+2R_2$ (op3). Can you find a single matrix which performs all three operations in order?
  \end{itemize}
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LU factorization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 2.6.

We begin by recalling the method of elimination matrices from the previous section. Given a system of linear equations, $A\mathbf{x}=\mathbf{b}$ we left-multiply the equation by a sequence of elmination matrices $E_i$ as follows.
\[E_k\cdots E_2E_1A\mathbf{x}=E_k\cdots E_2E_1\mathbf{b}
\]
At the end of the process, the left hand side is in its echelon (or upper triangular) form. In other words, we write the above equation as
\[U\mathbf{x}=E_k\cdots E_2E_1\mathbf{b}
\]

In this section we will use matrix algebra to ``divide'' the $E_i's$ back over to the left-hand side. While division doesn't necessarily make sense in matrix algebra, it turns out that the elementary matrices $E_i$ always have an ``inverse matrix'' $E_i^{-1}$ which has the opposite effect of $E_i$. For example, the following matrices cancel each other out.
\[E=\begin{bmatrix}1\\3&1\end{bmatrix},\quad
  E^{-1}=\begin{bmatrix}1\\-3&1\end{bmatrix}
\]
This allows us to rewrite the previous equation as
\[E_1^{-1}E_2^{-1}\cdots E_k^{-1}U\mathbf{x}=\mathbf{b}
\]

For the rest of this section, let's assume there are \emph{no row exchanges} needed in the elimination. Then all of the matrices $E_i^{-1}$ are \emph{lower triangular}, meaning all entries above the main diagonal are $0$. We thus arrive at the following conclusion. We can \emph{factor} $A$ as a product $LU$ and write the original system as
\[LU\mathbf{x}=\mathbf{b}
\]
where $L=E_1^{-1}E_2^{-1}\cdots E_k^{-1}$ and $U$ is the eliminated matrix.

There are two reasons to do this. First, just as in ordinary arithmetic, it is often revealing to factor something as a product of simpler things. Second, it expresses the given system is a composition of two triangular systems, which are much simpler to solve than general systems.

\begin{example}
  Factor the matrix $A=LU$.
  \[A=\begin{bmatrix}2&4&2\\1&5&2\\4&-1&9\end{bmatrix}
  \]
  The elimination matrices are
  \[E_1=\begin{bmatrix}1\\-\frac12&1\\&&1\end{bmatrix},\quad
    E_2=\begin{bmatrix}1\\&1\\-2&&1\end{bmatrix},\quad
    E_3=\begin{bmatrix}1\\&1\\&3&1\end{bmatrix}
  \]
  The inverse matrices are
  \[E_1^{-1}=\begin{bmatrix}1\\\frac12&1\\&&1\end{bmatrix}
    E_2^{-1}=\begin{bmatrix}1\\&1\\2&&1\end{bmatrix}
    E_3^{-1}=\begin{bmatrix}1\\&1\\&-3&1\end{bmatrix}
  \]
  We can combine these together to get $L$. The final answer is
  \[A=\begin{bmatrix}1&&\\\frac12&1&\\2&-3&1\end{bmatrix}
    \begin{bmatrix}2&4&2\\&3&1\\&&8\end{bmatrix}
  \]
  Finally we should check we are correct by multiplying $LU$ to get $A$.
\end{example}

Next if we have a factored system $LU\mathbf{x}=\mathbf{b}$ we can use the substitution $\mathbf{c}=U\mathbf{x}$ to break it into two triangular systems: $L\mathbf{c}=\mathbf{b}$ and then $U\mathbf{x}=\mathbf{c}$. We can solve the first using ``forth-solving'' and the the second using back-solving.

\begin{example}
  Consider the system
  \[\systeme{2x+4y+2z=10,x+5y+2z=11,4x-y+9z=2}
  \]
  By the work we have done previously, the system can be broken into two:
  \[\systeme{c=10,\frac12c+d=11,2c-3d+e=2}
    \qquad\text{and}\qquad
    \systeme{2x+4y+2z=c,3y+z=d,8z=e}
  \]
  The first can easily be solved forwards with solution $c=10$, $d=6$, $e=0$. We then plug these values into the second to solve backwards with solution $x=1$, $y=2$, $z=0$.
\end{example}

We close this section with the really cool part. Suppose the matrix $A$ models a given phenomenon, and you want to solve $A\mathbf{x}=\mathbf{b}$ over and over again for many different $\mathbf{b}$'s. If we had to perform elimination each time, we would be spending $O(n^3)$ operations each time. If on the other hand we factor $A=LU$ just once, then we can reduce every future instance of $A\mathbf{x}=\mathbf{b}$ to two back-solves, which consumes just $O(n^2)$ operations!

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Let $A$ be the following matrix.
  \[\begin{bmatrix}2&-3\\4&3\end{bmatrix}
  \]
  Find the elimination matrix $E$ that puts $A$ in upper triangular form $U$. Let $L$ be the inverse matrix $E^{-1}$ that ``undoes'' $E$. Write $A=LU$ and confirm that it is true.
  \item Let $A$ be the following matrix.
  \[\begin{bmatrix}1&1&1\\2&4&5\\3&5&2\end{bmatrix}
  \]
  \begin{enumerate}
    \item Use elimination to put $A$ into upper triangular form $U$. Keep track of the elimination matrices $E_1,E_2,E_3$.
    \item Write the inverse matrices $E_1^{-1},E_2^{-1},E_3^{-1}$ that undo the elimination matrices.
    \item Find the combined matrix $L=E_1^{-1}E_2^{-1}E_3^{-1}$.
    \item Write $A=LU$ and confirm that it is true.
    \item Consider the following system made from the entries of $A$:
    \[\systeme{x+y+z=2,2x+4y+5z=8,3x+5y+2z=2}
    \]
    Use your factorization $A=LU$ from part (d) to write two systems, $L\mathbf{c}=\mathbf{b}$ and $U\mathbf{x}=\mathbf{c}$.
    \item Solve the system $L\mathbf{c}=\mathbf{b}$ by forth-solving. Solve the system $U\mathbf{x}=\mathbf{c}$ by back-solving. Check that your solution for $\mathbf{x}$ is correct by plugging it into the original system $A\mathbf{x}=\mathbf{b}$.
  \end{enumerate}
\end{activity}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inverse matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 2.5.

In the previous section we have seen that while we cannot necessarily divide matrices, elimination matrices $E$ do have an ``inverse'' $E^{-1}$ which undoes the effect of $E$. In this section we will see that most square matrices $A$ have an inverse $A^{-1}$ which undoes multiplication by $A$.

The official definition of $A^{-1}$ is that it is a multiplicative inverse for $A$. First recall that $I$ denotes the identity matrix: the matrix with the same size as $A$, but with $1$'s down the diagonal and $0$'s everywhere else. The identity matrix is the multiplicative identity which means that $AI=IA=A$.

\begin{definition}
  If $A$ is any square matrix, its \emph{inverse} is the matrix $A^{-1}$ such that $AA^{-1}=A^{-1}A=I$.
\end{definition}

\begin{example}
  We have already seen that elimination matrices $A$ have an inverse. Let $A$ be the following matrix
  \[A=\begin{bmatrix}1&0\\3&1\end{bmatrix}
  \]
  Then we have
  \[A^{-1}=\begin{bmatrix}1&0\\-3&1\end{bmatrix}
  \]
  We can verify that $AA^{-1}=I$.
\end{example}

\begin{example}
  Other types of matrices have inverses too.
  \[A=\begin{bmatrix}2&5\\1&3\end{bmatrix},\qquad
    A^{-1}=\begin{bmatrix}3&-5\\-1&2\end{bmatrix}
  \]
  We can check that the two matrices are inverses by a direct calculation.
\end{example}


Not every matrix has an inverse. In the rest of the section we will talk about how to find out if a matrix has an inverse, and if it does, how to calculate what the inverse is.

Recall that we can use elimination matrices to make a given matrix upper triangular:
\[E_k\cdots E_1A=U
\]
This zeros out the entries below the pivots. The idea is to \emph{continue elimination} to zero out not only the entries below the pivots, but \emph{above} the pivots as well. We can also ensure the pivots themselves are all equal to $1$, which leaves just the identity matrix in the end.
\[E_m\cdots E_k\cdots E_1A=I
\]
The inverse of $A$ is now observed to be the product of all the elimination matrices.

In order to keep a running track of the product of the elimination matrices, we begin with a (super) augmented matrix containing $A$ and the identity $I$. We then perform a side-by-side elimination.
\begin{align*}
  [A&\mid I]\\
  [E_1A&\mid E_1]\\
    &\vdots\\
  [E_m\cdots E_1A&\mid E_m\cdots E_1]\\
  [I&\mid A^{-1}]
\end{align*}
When the left side is finally transformed to $I$, the inverse appears in the right side.

\begin{example}
  Let $A$ be the following matrix.
  \[A=\begin{bmatrix}1&4&3\\-1&-2&0\\2&2&3\end{bmatrix}
  \]
  Begin with the matrix
  \[\begin{bmatrix}
    1&4&3&\vline&1\\
    -1&-2&0&\vline&&1\\
    2&2&3&\vline&&&1
    \end{bmatrix}
  \]
  The elimination results in
  \[\begin{bmatrix}
    1&&&\vline&-\frac12&-\frac12&\frac12\\
    &1&&\vline&\frac14&-\frac14&-\frac14\\
    &&1&\vline&\frac16&\frac12&\frac16
    \end{bmatrix}
  \]
  We may confirm this is $A^{-1}$ by multiplying $AA^{-1}$ and getting $I$.
\end{example}

It is important to note that not every matrix $A$ has an inverse matrix! First of all the matrix has to be square. Next, in order for the method to work, we must have a (nonzero) pivot in every diagonal position. A square matrix with no inverse is called \emph{singular}.

\begin{example}
  The following matrix is singular.
  \[\begin{bmatrix}1&2&3\\1&2&4\\2&4&7\end{bmatrix}
  \]
  We can confirm this by using elimination until a row of zeros is revealed.
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Use elimination to find the inverse of each matrix, or else to show that it is singular.
  \begin{enumerate}
    \item $\begin{bmatrix}1&2\\3&4\end{bmatrix}$
    \item $\begin{bmatrix}\frac12&3\\2&12\end{bmatrix}$
    \item $\begin{bmatrix}1&0&3\\2&3&4\\1&0&2\end{bmatrix}$
  \end{enumerate}
  \item Find the inverse of the matrix (treat the letters $a,b$ as variables):
  \[\begin{bmatrix}1&a&0\\0&1&b\\0&0&1\end{bmatrix}
  \]
  \item Assume $A$ is an invertible $n\times n$ matrix and consider the system $A\mathbf x=\mathbf b$. Is it possible for the system to have no solution? Is it possible for the system to have infinitely many solutions?
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Column space}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 3.1.

In previous sections we have seen how to use elimination to decide whether a system of linear equations has zero, one, or infinitely many solutions. In this section we will explore the theory behind whether and when a system of linear equations has any solutions or not.

We will still be viewing a system of linear equations as a single matrix equation
\[A\mathbf{x}=\mathbf{b}
\]
If we view $A$ as a function mapping inputs $\mathbf{x}$ to outputs $\mathbf{b}$, then observe that $A\mathbf{x}=\mathbf{b}$ has solutions if and only if $\mathbf{b}$ is in the \emph{range} of $A$. This means we have to study the idea of the range of a matrix! Before we do that, it is best to introduce a big concept called a vector space.

\begin{definition}
  A \emph{vector space} is any collection of mathematical objects that may be scaled and added.
\end{definition}

The first and most important example of a vector space is $\R^n$, the set of vectors with $n$ components. (This is really a sequence of examples because each $n$ gives rise to a different example.) Recall that the key property of vectors in $\R^n$ is that they can be scaled and added together, so in our new terminology we may simply say that $\R^n$ is a vector space.

There are many other examples of vector spaces including polynomials, continuous functions, differentiable functions, matrices, and more. But in this class all the examples that we care about are the vector spaces $\R^n$ and the vector spaces that exist inside $\R^n$ called subspaces.

\begin{definition}
  A \emph{subspace} of $\R^n$ is a subset $S$ of $\R^n$ with the property that scaling and adding vectors in $S$ never ends up outside of $S$.
\end{definition}

There are always two fundamental ways to describe a subspace of $\R^n$. One way is using \emph{equations}. For example, the set of solutions to $x+y+z=0$ is a subspace of $\R^3$. If we look at two solutions $(1,1,-2)$ and $(-1,-2,3)$, the sum $(0,-1,1)$ is also a solution. It's important to know that not all equations work. For example the set of solutions to $2x+y=1$ is not a subspace of $\R^2$. If we look at the two solutions $(2,-3)$ and $(1,-1)$, the sum $(3,-4)$ is not a solution.

The second way to describe a subspace of $\R^n$ is using \emph{combinations}. If $\textbf{v}_1,\ldots,\textbf{v}_k$ are vectors then a \emph{combination} of $\textbf{v}_1,\ldots,\textbf{v}_k$ is any vector that can be obtained by scaling and adding them together.

\begin{definition}
  Given any matrix $A$ with column vectors $\mathbf{v}_1,\ldots\mathbf{v}_n$, the \emph{column space} of $A$ consists of all combinations $\mathbf{v}_1x_1+\cdots+\mathbf{v}_nx_n$ of the column vectors of $A$.
\end{definition}

Thus the column space of $A$ is precisely the set of of all vectors $\mathbf{b}$ such that $A\mathbf{x}=\mathbf{b}$ has a solution. The column space of $A$ is the range of $A$.

\begin{example}
  Consider the matrix:
  \[A=\begin{bmatrix}1&0\\4&3\\2&3\end{bmatrix}
  \]
  The column space of $A$ is a plane consisting of all combintations of the two column vectors. We can test whether the vector $(-1,-1,1)$ lies in the column space of $A$ by deciding whether the system is consistent:
  \[\begin{bmatrix}1&0&\vline&-1\\4&3&\vline&-1\\2&3&\vline&1\end{bmatrix}
  \]
  Elimination yields
  \[\begin{bmatrix}1&0&\vline&-1\\0&3&\vline&3\\0&0&\vline&0\end{bmatrix}
  \]  
  Thus the vector $(-1,-1,1)$ lies in the column space of $A$, as witnessed by the coefficients $(x,y)=(-1,1)$.
\end{example}

In the above example we checked whether a single vector $\mathbf{b}$ is in the column space of $A$. More generally we can use algebra to describe all vectors $\mathbf{b}$ in the column space of $A$.

\begin{example}
  Let $A$ be the matrix in the previous example. Find equations for the column space of $A$. The answer consists of one or more equations in the variables $b_1,b_2,b_3$ that determine when $A\mathbf{x}=\mathbf{b}$ has a solution. To solve this we set up an augmented matrix with unknowns on the right-hand side.
  \[A=\begin{bmatrix}
      1&0&\vline&b_1\\
      4&3&\vline&b_2\\
      2&3&\vline&b_3\\
    \end{bmatrix}
  \]
  Using elimination, we find that the system is consistent precisely when $b_3-2b_1=0$. This equation is the solution.
\end{example}

Note that when we are looking for the equations of a subspace, the right-hand side of the equations is always $0$. If we think about subspaces geometrically, this means that subspaces of $\R^n$ all have the shape of a point, line, plane, etc, passing through the origin. In fact, the dimension of the column space of $A$ corresponds to the number of pivots during elimination.

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item For each matrix $A$, find the column space of $A$. The answer consists of one or more equations in the variables $b_1,b_2,b_3$ that decide when $A\mathbf{x}=\mathbf{b}$ has a solution.
  \begin{enumerate}
    \item $A=\begin{bmatrix}1&4&2\\2&8&4\\-1&-4&-2\end{bmatrix}$\vspace{.3in}
    \item $A=\begin{bmatrix}1&4\\2&9\\-1&-4\end{bmatrix}$\vspace{.3in}
    \item $A=\begin{bmatrix}-2&1&4\\0&-5&-4\\1&-3&-4\end{bmatrix}$
  \end{enumerate}
  \item Decide whether each of the following is true or false. Explain your answers!
  \begin{enumerate}
    \item The matrices $A$ and $2A$ have the same column space
    % \item If the column space of the matrix $A$ is trivial (a point) then $A$ is trivial (the zero matrix)
    \item Given a matrix $A$, the set of vectors $\mathbf{b}$ such that $A\mathbf{x}=\mathbf{b}$ has a solution forms a vector space
    \item Given a matrix $A$ and a vector $\mathbf{b}$, the set of vectors $\mathbf{x}$ such that $A\mathbf{x}=\mathbf{b}$ forms a vector space
  \end{enumerate}
\end{activity}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Null space: the complete solution to $A\mathbf{x}=\mathbf{0}$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 3.2.

In the last section we introduced vector spaces, along with our primary example, the column space of a matrix. As it turns out, there are several more important subspaces that are associated with a given matrix $A$ (see the cover of the text!). In this session we introduce the null space.

\begin{definition}
  Given a matrix $A$, the \emph{null space} of $A$ is the set of all $\mathbf{x}$ such that $A\mathbf{x}=\mathbf{0}$.
\end{definition}

Thinking of $A$ as a function, the column space of $A$ is the range, and the null space is the set of zeros or roots.

Note that if $A$ is $m\times n$, then the null space of $A$ is a subset of $\R^n$. It is important for us to check that the null space is really a subspace of $\R^n$, that is, closed under scaling and addition: if $A\mathbf{x}=0$ then $A\alpha\mathbf{x}=\alpha A\mathbf{x}=\mathbf{0}$ too; if $A\mathbf{x}=\mathbf{0}$ and $A\mathbf{w}=\mathbf{0}$, then we have $A(\mathbf{x}+\mathbf{w})=A\mathbf{x}+A\mathbf{w}=\mathbf{0}$ too.

We want to find the null space of a given matrix. We begin with an example.

\begin{example}
  Consider the following matrix.
  \[A=\begin{bmatrix}1&2&2\\2&4&8\\3&6&10\end{bmatrix}
  \]
  As usual the first step is to perform elimination.
  \[A=\begin{bmatrix}1&2&2\\0&0&4\\0&0&0\end{bmatrix}
  \]
  Next identify the variables whose column has no pivot, called \emph{free variables}, and the variables whose column has a pivot, called \emph{basic variables}. Then solve for the basic variables \emph{in terms of} the free variables.
  \[\begin{bmatrix}x\\y\\z\end{bmatrix}
    =\begin{bmatrix}-2y\\y\\0\end{bmatrix}
  \]
\end{example}

When we find the null space, we can achieve some additional clarity if we eliminate both below and above the pivots, and also clear the pivots to $1$s. This is called the \emph{reduced row echelon form} (RREF) of the matrix.

\begin{example}
  Consider the following matrix.
  \[A=\begin{bmatrix}1&2&2&4\\3&8&6&16\end{bmatrix}
  \]
  The first step is to eliminate all the way to RREF.
  \[A=\begin{bmatrix}1&0&2&0\\0&1&0&2\end{bmatrix}
  \]
  Then solve for the basic variables in terms of the free variables.
  \[\begin{bmatrix}x\\y\\z\\w\end{bmatrix}
  =\begin{bmatrix}-2z\\-2w\\z\\w\end{bmatrix}
  \]
  Even better, by factoring out the free variables, we can write the solution in \emph{parametric vector form}.
  \[\begin{bmatrix}x\\y\\z\\w\end{bmatrix}
    =\begin{bmatrix}-2\\0\\1\\0\end{bmatrix}z
    +\begin{bmatrix}0\\-2\\0\\1\end{bmatrix}w
  \]
  Since the names of the parameters $z$ and $w$ are in some sense unnecessary information, we often write just the \emph{null space matrix} which is the matrix whose columns are the numeric vectors from the parametric vector form.
  \[N=\begin{bmatrix}-2&0\\0&-2\\1&0\\0&1\end{bmatrix}
  \]
  It is called the null space matrix because $AN=0=$ the zero matrix.
\end{example}

The solutions to $A\mathbf{x}=\mathbf{0}$ each provide a recipe for a combination of the columns of $A$ that gives $\mathbf{0}$.

\begin{example}
  Calculate the null space matrix of $A$.
  \[A=\begin{bmatrix}1&2&2&3\\2&4&8&10\\3&6&10&13\end{bmatrix}
  \]
  The solution is
  \[N=\begin{bmatrix}-2&-1\\1&0\\0&-1\\0&1\end{bmatrix}
  \]
\end{example}

The null space matrix $N$ is always as tall as $A$ was wide, and it has one column for every free variable. There is a trick for writing down the null space matrix $N$ once you know the RREF matrix $R$: (1) put an identity matrix into the free variable rows of $N$, and; (2) put the negatives of the free variable columns of $R$ into the basic variable rows of $N$.

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item The following matrices $A$ are in RREF. Find the complete solution to $A\mathbf{x}=\mathbf{0}$ in parametric form. Then write the null space matrix $N$ and check that $AN=0$.
  \begin{enumerate}\itemsep10pt
    \item $A=\begin{bmatrix}1&0&1\\0&1&-1\end{bmatrix}$
    \item $A=\begin{bmatrix}1&0&2&3\\0&1&4&5\\0&0&0&0\end{bmatrix}$
    \item $A=\begin{bmatrix}1&-3&0&4\\0&0&1&0\end{bmatrix}$
  \end{enumerate}
  \item For each system, write the coefficients matrix, find its RREF, and write the complete solution in parametric form.
  \begin{enumerate}\itemsep10pt
    \item \systeme{2x+3y-z=0,x+z=0}
    \item \systeme{x+3y+3z=0,2x+6y+9z=0,-x-3y+3z=0}
    \item \systeme[xyzw]{x+2.5y+z+w=0,2x+5y+3z-2w=0}
  \end{enumerate}
  \item The following system is from 2(b), but modified to have a nonzero right-hand side:
  \[\systeme{x+3y+3z=2,2x+6y+9z=7,-x-3y+3z=4}
  \]
  \begin{enumerate}
    \item Using ordinary elimination and back-solving, find \emph{one} solution to this system.
    \item Add your solution for 3(a) to one of the numeric vectors from your solution to 2(b). Check that the result is a solution to this system too. How can you explain this?
  \end{enumerate}
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The complete solution to $A\mathbf{x}=\mathbf{b}$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 3.3.

In the previous section we showed how to find the complete solution to a system of linear equations $A\mathbf{x}=\mathbf{0}$. A system with $\mathbf{0}$ on the right-hand side is called a \emph{homogeneous} system, just as in differential equations. We will now use this as a stepping stone to solving a general system of linear equations, $A\mathbf{x}=\mathbf{b}$.

Recall that the solutions to $A\mathbf{x}=\mathbf{b}$ form the \emph{null space} of $A$, which is a vector subspace of $\R^n$. It turns out that solutions to the system $A\mathbf{x}=\mathbf{0}$ form a \emph{translation} of the null space of $A$.

To get the idea, let's look at the case of one equation in two variables. For example, first consider the \emph{homogeneous} equation:
\[2x+3y=0
\]
The solutions to this equation form a line through the origin, a subspace of $\R^2$. We can plot it:

\begin{center}
  \begin{tikzpicture}
    \draw[gray!30,very thin,dashed] (-1.9,-1.9) grid (2.9,1.9);
    \draw[->,thick] (-2,0)--(3,0);
    \draw[->,thick] (0,-2)--(0,2);
    \draw[domain=-1:2] plot (\x,-2/3*\x);
    \node[right] at (2,-4/3) {$2x+3y=0$};
  \end{tikzpicture}
\end{center}

Next consider an \emph{inhomogeneous} equation with the same left-hand side:
\[2x+3y=3
\]
The solutions to this equation form a line, but not through the origin. It is the same line as above but shifted to the right by $3/2$ (or up by $1$).

\begin{center}
  \begin{tikzpicture}
    \draw[gray!30,very thin,dashed] (-1.9,-1.9) grid (2.9,1.9);
    \draw[->,thick] (-2,0)--(3,0);
    \draw[->,thick] (0,-2)--(0,2);
    \draw[domain=-1:2] plot (\x,-2/3*\x+1);
    \node[right] at (2,-1/3) {$2x+3y=3$};
  \end{tikzpicture}
\end{center}

We can redo this problem in matrix notation. We are using the matrix $A=\begin{bmatrix}2&3\end{bmatrix}$. First we find the general solution to $A\mathbf{x}=\mathbf{0}$, that is, the null space of $A$:
\[\mathbf{x}_{\mathrm{null}}
  =\begin{bmatrix}-3/2\\1\end{bmatrix}y
\]
The null space is the line through the origin that we found above. The general solution to $A\mathbf{x}=[3]$ is then:
\[\mathbf{x}_{\mathrm{general}}
  =\begin{bmatrix}3/2\\0\end{bmatrix}
  +\begin{bmatrix}-3/2\\1\end{bmatrix}y
\]
which is the line translated away from the origin that we found above.

In general, we will find that the general solution to a system of linear equations $A\mathbf{x}=\mathbf{b}$ will be equal to a single vector (the particular solution) added to the vectors in the null space (the parametric vector form). This strongly mirrors the method from systems of linear differential equations.

\begin{example}
  Consider the system $A\mathbf{x}=\mathbf{b}$:
  \[\begin{bmatrix}
      1&3&0&2\\0&0&1&4\\1&3&1&6
    \end{bmatrix}
    \mathbf{x}=
    \begin{bmatrix}
      1\\6\\7
    \end{bmatrix}
  \]
  \begin{itemize}
  \item Step one. Eliminate the full system to RREF.
    \[R=\begin{bmatrix}
        1&3&0&2&\vline&1\\
        &&1&4&\vline&6\\
        &&&&\vline&
      \end{bmatrix}
    \]
  \item Step two. Find the null space. Forget about the right-hand side briefly, pretend it is zeros, and find the null space of $A$ in \emph{parametric vector form}.
  \[\mathbf{x}_{\mathrm{null}}
  =\begin{bmatrix}-3\\1\\0\\0\end{bmatrix}x_2
  +\begin{bmatrix}-2\\0\\-4\\1\end{bmatrix}x_4
  \]
  \item Step three. Find the particular solution to the original system by setting the free variables to zero. Note there is a trick here: put zeros in the free variable rows and the RHS of $R$ in the free variable rows.
  \[\mathbf{x}_{\mathrm{partic}}
  =\begin{bmatrix}1\\0\\6\\0\end{bmatrix}
  \]
  \item Step four. Write the solution; it is just the sum of the particular and general solutions.
    \[\mathbf{x}_{\mathrm{general}}
      =\begin{bmatrix}1\\0\\6\\0\end{bmatrix}
      +\begin{bmatrix}-3\\1\\0\\0\end{bmatrix}x_2
      +\begin{bmatrix}-2\\0\\-4\\1\end{bmatrix}x_4
    \]
  \end{itemize}
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item For each system, solve or state that it is inconsistent. If the system is consistent, find the null space in parametric vector form, the particular solution, and finally write the general solution.
  \begin{enumerate}\itemsep10pt
    \item $\systeme{x+z=1,2x+3y-z=1}$
    \item $\systeme{x+3y+3z=1,2x+6y+9z=5,-x-3y+3z=5}$
    \item $\begin{bmatrix}1&3&1&2\\2&6&4&8\\0&0&2&4\end{bmatrix}\mathbf{x}
      =\begin{bmatrix}1\\3\\1\end{bmatrix}$
    \item $\begin{bmatrix}0&1\\0&-3\end{bmatrix}\mathbf{x}
      =\begin{bmatrix}2\\-6\end{bmatrix}$
  \end{enumerate}
  \item Give an example of a system with two equations and three variables whose general solution is:
  \[\mathbf{x}=
  \begin{bmatrix}2\\4\\0\end{bmatrix}+\begin{bmatrix}1\\1\\1\end{bmatrix}z
  \]
  \item Suppose that $A$ is $m\times n$ and has $r$ many pivots. Assume you know the system $A\mathbf x=\mathbf b$ is consistent. What is the \emph{dimension} of the set of solutions? (This is equivalent to asking how many free variables there are in the general solution.) Explain your answer.
\end{activity}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Subspaces, orthogonality, and applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Independence and basis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 3.4.

Recall that the column space of a matrix $A$ consists of all combinations of the columns of $A$. Sometimes we say the columns are a \emph{spanning set} for the column space. The idea is if I want to tell you about a plane, I only need to tell you two vectors in the plane (a spanning set) and not every vector in the plane. You can deduce the rest using combinations.

We have seen that sometimes not all the columns are necessary to form a spanning set. For example, consider the matrix:
\[A=\begin{bmatrix}1&1&3\\2&0&2\\3&1&5\end{bmatrix}
\]
Notice that the third column of $A$ is a combination of the first two. The first two columns already form a spanning set, making the third column redundant. Geometrically, the three columns of $A$ point in three distinct directions, but never manage to break outside of a plane.

\begin{definition}
  We say that a set of vectors is \emph{linearly dependent} if one of the vectors is a combination of the others. Otherwise we say the set is \emph{linearly independent}. If $V$ is a vector space, then a \emph{basis} for $V$ is a linearly independent set of vectors whose combinations span all of $V$.
\end{definition}

A basis for a vector space has ``just the right number'' of vectors. A basis for a plane will have two vectors, for a three-dimensional vector space will have three vectors, etc. In fact we can take the definition of \emph{dimension} as the number of vectors in a basis.

Turning to practical matters, suppose we are given a matrix $A$, and asked to find bases for its column space and null space. Beginning with the column space, the goal is to remove the redundant columns until we are left with the right number, the basis. There is a simple rule to determine which columns should be in the basis and which columns are redundant: The basic (non-free) variable columns belong in the basis, and the free variable columns are redundant.

\begin{example}
  Consider the following matrix:
  \[A=\begin{bmatrix}1&2&0&3\\1&2&-1&-1\\2&4&-1&2\end{bmatrix}
  \]
  The RREF is as follows:
  \[R=\begin{bmatrix}1&2&0&3\\0&0&1&4\\&&&\end{bmatrix}
  \]
  The RREF shows exactly why the free variable columns are redundant: the entries reveal how they are combinations of earlier columns. The basis for the column space consists of the basic variable columns from \emph{the original matrix}.
  \[\text{basis for $C(A)$: }\begin{bmatrix}1\\1\\2\end{bmatrix},
  \begin{bmatrix}0\\-1\\-1\end{bmatrix}
  \]
\end{example}

We also wish to find a basis for the null space of $A$. In fact we have already done this: the basis simply consists of the the column vectors in the paramteric vector form.

\begin{example}
  The basis for the null space of the matrix in the previous problem consists of the column vectors in the parametric vector form of the solution to $A\mathbf{x}=\mathbf{0}$:
  \[\text{basis for $N(A)$: }\begin{bmatrix}2\\1\\0\\0\end{bmatrix},
  \begin{bmatrix}-3\\0\\-4\\1\end{bmatrix}
  \]
\end{example}

To summarize: To find a basis for the column space of $A$, we eliminate to REF, identify the pivot columns, and select those columns from the original matrix $A$. To find a basis for the null space of $A$, we find the column vectors in the  parametric vector form or the columns of the null space matrix.

We can apply the same reasoning to remove the redundant elements from any set of vectors, and find a linearly independent subset with the same span.

\begin{example}
  Find a basis for the subspace of $\R^3$ spanned by the given vectors. What is the dimension of the subspace?
  \[\begin{bmatrix}1\\0\\1\end{bmatrix},\ 
    \begin{bmatrix}2\\0\\2\end{bmatrix},\ 
    \begin{bmatrix}3\\0\\3\end{bmatrix},\ 
    \begin{bmatrix}2\\1\\2\end{bmatrix}
  \]
  We can squash these vectors into the columns of a matrix and find the RREF:
  \[R=\begin{bmatrix}1&2&3&0\\0&0&0&1\\&&&\end{bmatrix}
  \]
  The basic columns are $1,4$, so the basis consists of the first and fourth vector:
  \[\text{basis: }\begin{bmatrix}1\\0\\1\end{bmatrix},\ 
  \begin{bmatrix}2\\1\\2\end{bmatrix}
  \]
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Find a basis for the null space and column space of each matrix.
  \begin{enumerate}
    \item $A=\begin{bmatrix}1&-1&3\\-2&2&-6\end{bmatrix}$
    \item $A=\begin{bmatrix}1&2&4\\0&1&-2\end{bmatrix}$
    \item $A=\begin{bmatrix}1&3&0&-1\\2&6&1&-4\\1&3&-2&3\end{bmatrix}$
    \item $A=\begin{bmatrix}2&0&-1&4&2\\4&1&0&2&-1\\2&1&1&-2&-3\end{bmatrix}$
  \end{enumerate}
  \item Determine whether the set of vectors is independent or dependent. If it is dependent, eliminate the (and only the) redundant vectors to find an independent subset.
  \begin{enumerate}
    \item $\begin{bmatrix}1\\0\\1\end{bmatrix},\quad
      \begin{bmatrix}1\\1\\1\end{bmatrix},\quad
      \begin{bmatrix}1\\1\\2\end{bmatrix},\quad
      \begin{bmatrix}2\\3\\6\end{bmatrix}$
      \vspace{6pt}
    \item $\begin{bmatrix}2\\0\\0\\4\end{bmatrix},\quad
      \begin{bmatrix}3\\6\\0\\6\end{bmatrix},\quad
      \begin{bmatrix}4\\7\\0\\8\end{bmatrix},\quad
      \begin{bmatrix}1\\0\\9\\2\end{bmatrix}$
  \end{enumerate}
  \item Suppose you are given a set of four vectors in $\R^3$. Your friend tells you the set cannot possibly be independent. Decide whether this true or false, and explain why.
  % \item
  % \begin{enumerate}
  %   \item Give an example of three vectors $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ such that (1) the vectors are linearly dependent, and (2) if you remove any one of them the remaining two are linearly independent.
  %   \item Give an example of three vectors $\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3$ such that (1) the vectors are linearly dependent; (2) if you remove $\mathbf{w}_1$ then the remaining two are linearly independent, and; (3) if you remove $\mathbf{w}_2$ then the remaining two are linearly dependent.
  % \end{enumerate}
\end{activity}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The fundamental subspaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 3.5.

We have previously discussed two ``fundamental'' subspaces associated with a given matrix $A$: the column space and the null space. The column space is the span of the columns, the null space is the combinations of the columns that give zero. It should not surprise us that there are two more fundamental subspaces corresponding to the rows.

We can view the rows of a matrix as columns of the ``transpose'' matrix.

\begin{definition}
  Given a matrix $A$ we form the \emph{transpose} matrix $A^T$ as follows: The rows of $A^T$ are the columns of $A$ and the columns of $A^T$ are the rows of $A$.
\end{definition}

\begin{example}
  \[\begin{bmatrix}1&2&3\\4&5&6\end{bmatrix}^T
    =\begin{bmatrix}1&4\\2&5\\3&6\end{bmatrix}
  \]
\end{example}

The four fundamental subspaces corresponding to a matrix $A$ are the following.
\begin{itemize}
  \item $C(A)$, the \emph{column space}, the span (combinations) of the columns of $A$
  \item $N(A)$, the \emph{null space}, the solutions to $A\mathbf{x}=\mathbf{0}$ (the combinations of the columns that give zero, also the equations of the row space)
  \item $R(A)$, the \emph{row space}, the span (combinations) of the rows of $A$
  \item $LN(A)$, the \emph{left null space}, the solutions to $\mathbf{y}^TA=\mathbf{0}$ (the combinations of the rows that give zero, also the equations of the column space)
\end{itemize}

The four subspaces are depicted on the cover of our book.

We wish to find bases and dimensions of the four fundamental subspaces corresponding to $A$. We first need to calculate the RREF of $A$ augmented with an unknown right-hand side $\mathbf{b}$. Then we have:

\begin{itemize}
  \item $C(A)$: The basis consists of the columns of the \emph{original matrix} corresponding to basic columns of the RREF matrix.
  \item $N(A)$: The basis consists of the vectors in the parametric vector solution to $A\mathbf{x}=\mathbf{0}$. (If there are no free variables, the basis is $\emptyset$.)
  \item $R(A)$: The basis consists of the nonzero rows of the \emph{RREF matrix}.
  \item $LN(A)$: The basis consists of the coefficients of the equations in the $b_i$'s corresponding to zero rows of the RREF matrix. (If there are no zero rows, the basis is $\emptyset$.)
\end{itemize}

\begin{example}
  Consider the matrix $A$ below. Find bases and dimensions of the four fundamental subspaces.
  \[A=\begin{bmatrix}
      1&3&0&5\\2&6&1&16\\5&15&0&25
    \end{bmatrix}
  \]
  We first augment the matrix with an unknown right-hand side $\mathbf{b}$, and find RREF.
  \[R=\begin{bmatrix}
    1&3&0&5&\vline&b_1\\
    &&1&6&\vline&b_2-2b_1\\
    &&&&\vline&b_3-5b_1
    \end{bmatrix}
  \]
  % E=\begin{bmatrix}1&&\\-2&1&\\-5&&1\end{bmatrix}
  The four bases are thus:
  \[\text{C(A): }\begin{bmatrix}1\\2\\5\end{bmatrix},
    \begin{bmatrix}0\\1\\0\end{bmatrix},\quad
    \text{N(A): }\begin{bmatrix}-3\\1\\0\\0\end{bmatrix},
    \begin{bmatrix}-5\\0\\-6\\1\end{bmatrix},\quad
    \text{R(A): }\begin{bmatrix}1\\3\\0\\5\end{bmatrix},
    \begin{bmatrix}0\\0\\1\\6\end{bmatrix},\quad
    \text{LN(A): }\begin{bmatrix}-5\\0\\1\end{bmatrix}
  \]
  We can also see that $C(A)$ is a $2$-dimensional subspace of $\R^3$, $N(A)$ is a $2$-dimensional subspace of $\R^4$, $R(A)$ is a $2$-dimensional subspace of $\R^4$, and $LN(A)$ is a $1$-dimensional subspace of $\R^3$.
\end{example}

If $A$ is $m\times n$, the dimensions of the four fundamental subspaces of $A$ can always be calculated $m$, $n$, and the number of pivots $r$.
\begin{itemize}
  \item $C(A)$ is an $r$-dimensional subspace of $\R^m$
  \item $N(A)$ is an $n-r$-dimensional subspace of $\R^n$
  \item $R(A)$ is an $r$-dimensional subspace of $\R^n$
  \item $LN(A)$ is an $m-r$-dimensional subspace of $\R^m$
\end{itemize}
The number of pivots $r$ is called the \emph{rank} of the matrix $A$. The rank is the dimension of the range of $A$, and thus in some sense it measures the ``size'' of $A$.

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item For each of the following matrices $A$, first find the RREF (with $b_i$'s on the right), and use this to find bases for all four fundamental subspaces corresponding to $A$.
  \begin{enumerate}
    \item $A=\begin{bmatrix}
      2&4&0&1\\
      1&2&3&4
    \end{bmatrix}$
    \item $A=\begin{bmatrix}1&3&0&-1\\
      2&6&1&-4\\
      1&3&-2&3
    \end{bmatrix}$
    \item $A=\begin{bmatrix}
      -2&1&0&4&1\\
      4&-2&1&3&1\\
      2&-1&1&7&2
    \end{bmatrix}$
  \end{enumerate}
  \item
  \begin{enumerate}
    \item $A$ is a $3\times 5$ matrix with $2$ pivots. What are the dimensions of all four fundamental subspaces corresponding to $A$? Explain your reasoning.
    \item $A$ is a matrix whose null space is a $3$-dimensional subspace of $\R^5$ and whose left-null space is a subspace of $\R^7$. What are the dimensions of all four fundamental subspaces corresponding to $A$? Explain your reasoning.
  \end{enumerate}
  \item Fill in the blanks and explain: Any vector in the row space times any vector in the \underline{\hspace{.5in}} space gives $\mathbf{0}$. Any vector in the column space times any vector in the \underline{\hspace{.5in}} space gives $\mathbf{0}$.
  % \item Let $P$ be the plane with equation $\begin{bmatrix}2&3&-1\end{bmatrix}\mathbf{x}=0$. Find all solutions to the equation in parametric vector form. Which of the four subspaces is this? Recall that the \emph{normal line} is perpendicular to the plane and passes through $(2,3,-1)$. Which of the four subspaces is this?
\end{activity}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Orthogonality and fundamental subspaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 4.1.

In previous sections we saw that solving systems of linear equations is related to two ``fundamental'' subspaces of a matrix: the column space (which determines when it is consistent), and the null space (which are the solutions up to a translation). We then defined two more fundamental subspaces, the row space and the left null space. We saw that the four spaces are related to each other. In this section we reveal one more special property of the four subspaces.

\begin{definition}
  Vectors $\mathbf{v},\mathbf{w}$ are \emph{orthogonal}, written $\mathbf{v}\perp\mathbf{w}$, if they are perpendicular (at 90 degree angles to one another). Vector spaces $V,W$ in $\R^n$ are \emph{orthogonal}, written $V\perp W$, if every vector of $V$ is orthogonal to every vector of $W$.
\end{definition}

You may recall from previous courses that orthogonality is related to the dot product of vectors. In linear algebra we write the dot product as $\mathbf{v}^T\mathbf{w}$ instead of $\mathbf{v}\cdot\mathbf{w}$.

\begin{lemma}
  Vectors $\mathbf{v},\mathbf{w}$ are orthogonal if and only if $\mathbf{v}^T\mathbf{w}=0$.
\end{lemma}

\begin{proof}
  Observe that $\mathbf{v},\mathbf{w}$ are orthogonal if and only if the triangle with corners $\mathbf{0},\mathbf{v},\mathbf{w}$ is a right triangle. By the Pythagorean theorem, this is the case if and only if
  \[|\mathbf{v}|^2+|\mathbf{w}|^2=|\mathbf{v-w}|^2
  \]
  We now use the distance formula, which states that the length of a vector is $|\mathbf{v}|=\sqrt{\sum v_i^2}$ (the distance formula is yet another version of the Pythagorean theorem). Thus the previous equation is equivalent to
  \[\sum v_i^2+\sum w_i^2=\sum(v_i-w_i)^2
  \]
  Applying the distributive property term-by-term to the right-hand side, we arrive at:
  \[\sum v_i^2+\sum w_i^2=\sum v_i^2-\sum 2v_iw_i+\sum w_i^2
  \]
  Most terms cancel, leaving $\sum v_iw_i=0$, or in other words, $\mathbf{v}^T\mathbf{w}=0$.
\end{proof}

\begin{example}
  The vector $(1,5,-7,-1)$ is orthogonal to the vector $(2,1,1,0)$. It is also orthogonal to the vector $(1,2,1,4)$. By the same reasoning, we can also say that the line $V$ through $(1,5,-7,-1)$ is orthogonal to the plane $W$ with basis $(2,1,1,0),(1,2,1,4)$.
\end{example}

For any subspace $V$, there is always a largest subspace $W$ such that $V,W$ are orthogonal.

\begin{definition}
  If $V$ is a subspace of some Euclidean space $\R^n$, its \emph{orthogonal complement} $V^\perp$ is the subspace consisting of all vectors $\mathbf{w}$ that are orthogonal to all vectors in $V$.
\end{definition}

\begin{example}
  If $V$ is the line in $\R^3$ that passes through the vector $(1,-1,2)$, then the orthogonal complement of $V$ would be the vectors $(x,y,z)$ such that
  \[\begin{bmatrix}1&-1&2\end{bmatrix}
    \begin{bmatrix}x\\y\\z\end{bmatrix}=0
  \]
  In other words, the orthogonal compleement is the set of solutions to $x-y+2z=0$, or in other words, the plane parameterized by
  \[\begin{bmatrix}1\\1\\0\end{bmatrix}y
    +\begin{bmatrix}-2\\0\\1\end{bmatrix}z
  \]
  It makes sense that the orthogonal complement of a line in $\R^3$ would be a plane---the dimensions of the two subspaces should add to $3$.
\end{example}

We saw from the example that if $A=\begin{bmatrix}1&-1&2\end{bmatrix}$ then the null space of $A$ is the orthogonal compelement of the row space of $A$. Strang states this is the fundamental theorem of linear algebra.

\begin{theorem}[Fundamental Theorem of Linear Algebra]
  Given any matrix $A$, the row space and null space are orthogonal complements of each other, and the column space and left-null space are orthogonal compelements of each other.
\end{theorem}

This theorem completes our understanding of the four fundamental subspaces. It also gives us a general method to calculate orthogonal complements.

\begin{example}
  Let $V$ be the subspace of $\R^4$ with the basis below. Find a basis for $V^\perp$.
  \[\text{basis for $V$: }\begin{bmatrix}1\\3\\0\\1\end{bmatrix},\quad
    \begin{bmatrix}1\\1\\2\\1\end{bmatrix}
  \]
  We solve this using the FTLA as follows: put the given vectors into the rows of a matrix, and then find the null space of the matrix. Here are the matrix and the final RREF.
  \[A=\begin{bmatrix}1&3&0&1\\1&1&2&1\end{bmatrix}\quad\rightarrow\quad
  R=\begin{bmatrix}1&0&3&1\\0&1&-1&0\end{bmatrix}
  \]
  The solution is just the basis for the null space, which is:
  \[\text{basis for $V^\perp$: }\begin{bmatrix}-3\\1\\1\\0\end{bmatrix},\quad
  \begin{bmatrix}-1\\0\\0\\1\end{bmatrix}
  \]
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Let $A$ be the matrix $A=\begin{bmatrix}2&3\\4&6\end{bmatrix}$.
  \begin{itemize}
    \item Find bases for the four fundamental subspaces corresponding to $A$.
    \item On a single set of axes, graph the row space and null space.
    \item On a second set of axes, graph the column space and left-null space.
    \item Who is orthogonal to whom and why?
  \end{itemize}
  \item In each problem, let $V$ be the subspace with the given basis. Find a basis for $V^\perp$.
  \begin{itemize}
    % \item Basis for $V$: $\begin{bmatrix}2\\-1\\\frac12\end{bmatrix}$
    \item Basis for $V$: $\begin{bmatrix}-1\\1\\2\\0\end{bmatrix}$
    \item Basis for $V$: $\begin{bmatrix}1\\2\\-5\\8\end{bmatrix},\quad \begin{bmatrix}-1\\-1\\3\\-5\end{bmatrix}$
  \end{itemize}
  \item 
    \begin{itemize}
      \item Find a vector orthogonal to the plane spanned by the vectors $(1,3,4)$ and $(5,2,7)$.
      \item Find a vector orthogonal to the plane with the equation $3x-y+2z=0$.
    \end{itemize}
  % \item Consider the vector $(2,3)$ in $\R^2$. What is the closest vector to $(2,3)$ on the $x$-axis? Draw a right triangle that would convince someone your answer is correct.
\end{activity}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Orthogonal projections}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 4.2.

Given a system of linear equations $A\mathbf{x}=\mathbf{b}$, if it is consistent we know how to solve it. If it is inconsistent, $\mathbf{b}$ isn't in the column space of $A$ and we can't find any solution. In this case, we can still ask for a \emph{best approximate solution} $\mathbf{x}$ that makes $A\mathbf{x}$ and $\mathbf{b}$ as close as possible. The method is to replace $\mathbf{b}$ with the \emph{nearest vector} $\mathbf{p}$ that is in the column space of $A$ and to solve $A\hat{\mathbf{x}}=\mathbf{p}$ instead.

Geometrically, if $A\mathbf{x}=\mathbf{b}$ is inconsistent, and $\mathbf{b}$ doesn't lie in the column space of $A$, we want to ``project'' $\mathbf{b}$ to the column space, or in other words find its shadow.

\begin{figure}[h]
  [Picture]
\end{figure}

It is not difficult to see that an optimal solution is achieved exactly when the error vector $\mathbf{b}-A\hat{\mathbf{x}}$ is orthogonal to the column space of $A$.

The theory for finding $\hat{\mathbf{x}}$ and $\mathbf{p}$ is known as \emph{orthogonal projection} and runs as follows. Given an inconsistent system $A\mathbf{x}=\mathbf{b}$, we set the error vector orthogonal to the columns of $A$:
\[A^T(\mathbf{b}-A\hat{\mathbf{x}})=0
\]
This simplifies to:
\[A^TA\hat{\mathbf{x}}=A^T\mathbf{b}
\]
We then solve this modified system in the usual way to find the best approximate solution $\hat{\mathbf{x}}$. Finally we can find the \emph{projection} vector $\mathbf{p}$ by multiplying $A\hat{\mathbf{x}}$.

\begin{example}
  Let $A\mathbf{x}=\mathbf{b}$ be the inconsistent system below. Find the best approximate solution $\hat{\mathbf{x}}$ and the projection $\mathbf{p}$ of $\mathbf{b}$ to $C(A)$.
  \[\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix}\mathbf{x}
    =\begin{bmatrix}6\\0\\0\end{bmatrix}
  \]
  The first step is to make the modified system $A^TA\hat{\mathbf{x}}=A^T\mathbf{b}$. It turns out to be:
  \[\begin{bmatrix}3&3\\3&5\end{bmatrix}\hat{\mathbf{x}}
  =\begin{bmatrix}6\\0\end{bmatrix}
  \]
  The second step is to solve using old-fashioned elimination and back-solving. The third step is to find the projection vector $\mathbf{p}=A\hat{\mathbf{x}}$. The solutions are:
  \[\hat{\mathbf{x}}=\begin{bmatrix}5\\-3\end{bmatrix},\quad 
  \mathbf{p}=\begin{bmatrix}5\\2\\-1\end{bmatrix}
  \]
\end{example}

The situation is somewhat simpler when $A$ is just a single vector $\mathbf{a}$. In this case we are projecting $\mathbf{b}$ onto the line through $\mathbf{a}$. Once again, we set the error vector $\mathbf{b}-\mathbf{a}\hat x$ orthogonal to $\mathbf{a}$:
\[\mathbf{a}^T(\mathbf{b}-\mathbf{a}\hat{x})=0
\]
This time we solve explicitly to obtain
\[\hat x=\mathbf{a}^T\mathbf{b}/\mathbf{a}^T\mathbf{a}
\]
We can also explicitly find the projection
\[\mathbf{p}=\mathbf{a}\hat{x}
  =\frac{\mathbf{a}^T\mathbf{b}}{\mathbf{a}^T\mathbf{a}}\mathbf{a}
\]

\begin{example}
  Find the projection of $\mathbf{b}$ onto the line through $\mathbf{a}$.
  \[\mathbf{a}=(1,2,3),\quad \mathbf{b}=(4,6,4)
  \]
  The solution is:
  \[\mathbf{p}=\frac{\begin{bmatrix}1&2&3\end{bmatrix}
  \begin{bmatrix}4\\6\\4\end{bmatrix}}
  {\begin{bmatrix}1&2&3\end{bmatrix}\begin{bmatrix}1\\2\\3\end{bmatrix}}
  \begin{bmatrix}1\\2\\3\end{bmatrix}
  =\frac{28}{14}\begin{bmatrix}1\\2\\3\end{bmatrix}
  =\begin{bmatrix}2\\4\\6\end{bmatrix}
  \]
\end{example}

We close by mentioning that given any matrix $A$, there is a single \emph{projection matrix} $P$ which carries vectors $\mathbf{b}$ to their projections $\mathbf{p}$ in the column space of $A$. To find a formula for $P$, we can use matrix algebra to write $\hat{\mathbf{x}}=(A^TA)^{-1}A^T\mathbf{b}$ and $\mathbf{p}=A\hat{\mathbf{x}}=A(A^TA)^{-1}A^T\mathbf{b}$. In other words we see that:
\[P=A(A^TA)^{-1}A^T
\]

\begin{example}
  Returning to the first example, we can calculate $P$.
  \[P=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix}
  \begin{bmatrix}3&3\\3&5\end{bmatrix}^{-1}
  \begin{bmatrix}1&1&1\\0&1&2\end{bmatrix}
  =\frac16\begin{bmatrix}5&2&-1\\2&2&2\\-1&2&5\end{bmatrix}
  \]
  We see that this matrix multiplies $\mathbf{b}$ to give the projection $\mathbf{p}$:
  \[\frac16\begin{bmatrix}5&2&-1\\2&2&2\\-1&2&5\end{bmatrix}
  \begin{bmatrix}6\\0\\0\end{bmatrix}
  =\begin{bmatrix}5\\2\\-1\end{bmatrix}
  \]
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Let $A\mathbf{x}=\mathbf{b}$ be the inconsistent system below. Find the best approximate solution $\hat{\mathbf{x}}$ and the projection $\mathbf{p}$ of $\mathbf{b}$ to $C(A)$. Finally find the projection matrix $P$ corresponding to $C(A)$.
  \begin{enumerate}
    \item $\begin{bmatrix}1&1\\0&1\\0&0\end{bmatrix}
      \mathbf{x}=\begin{bmatrix}2\\3\\4\end{bmatrix}$
    \item $\begin{bmatrix}1&1\\1&1\\0&1\end{bmatrix}
      \mathbf{x}=\begin{bmatrix}4\\6\\6\end{bmatrix}$ % new try on RHS
  \end{enumerate}
  \item Find the projection $\mathbf{p}$ of $\mathbf{b}$ onto the line through $\mathbf{a}$. In each case confirm that $\mathbf{b}-\mathbf{p}$ is orthogonal to $\mathbf{a}$.
  \begin{enumerate}
    \item $\mathbf a=\begin{bmatrix}1\\1\\1\end{bmatrix}$,
      $\mathbf b=\begin{bmatrix}1\\2\\2\end{bmatrix}$
    \item $\mathbf a=\begin{bmatrix}1\\2\\0\\1\end{bmatrix}$,
      $\mathbf b=\begin{bmatrix}1\\0\\1\\0\end{bmatrix}$
  \end{enumerate}
  \item When projecting $\mathbf{b}$ onto the column space of a matrix $A$, which of the four subspaces contains the projection $\mathbf{p}$? Which of the four subspaces contains the error vector $\mathbf{b}-\mathbf{p}$? Explain your answers.
\end{activity}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear modeling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 4.3.

In the previous section, we described how to find the orthogonal projection of a given vector $\mathbf{b}$ onto a vector subspace. The projection vector $\mathbf{p}$ is optimal in the sense that it is as close as possible to $\mathbf{b}$.

This concept has an enormous application in statistics, where it is called ``least squares optimization''. We often see data sets which, when plotted, do not form a line, but instead have a visible ``trend''. Given an arbitrary data set, we wish to find the line that most closely fits the data. We will use the fact that the possible data sets form a vector space, and the linear data sets form a subspace. We will take a given data set and \emph{project} it onto the subspace consisting of linear data sets! This is a beautiful example illustrating of the power of abstract mathematics!

Here is how it works. Given a sequence of data points $(t_1,b_1),\ldots,(t_n,b_n)$, the data lie on a line of the form $C+Dt=y$ if and only if the following system is consistent:
\[\begin{cases}1C+t_1 D=b_1\\\quad\vdots\\1C+t_nD=b_n\end{cases}
\]
If the system is not consistent we use the method of orthogonal projection, replacing $\mathbf{b}$ with $\mathbf{p}$ and finding the best approximate solutions $\hat C,\hat D$ for $C,D$.

\begin{example}
  Recall the example that we gave on the very first day of class. Given the following measurements, find the line that best fits the data.
  \[(2,3),(3,5),(6,6)
  \]
  The data lie on a line if and only if the system is consistent:
  \[\systeme[CD]{C+2D=3,C+3D=5,C+6D=6}
  \]
  Write it as a matrix equation:
  \[\begin{bmatrix}1&2\\1&3\\1&6\end{bmatrix}
    \mathbf{C}=\begin{bmatrix}3\\5\\6\end{bmatrix}
  \]
  The data do not lie on a line, so the system is inconsistent. Instead we find the best approximate solution. We proceed with $A^TA\hat{\mathbf{c}}=A^T\mathbf{b}$:
  \[\begin{bmatrix}3&11\\11&49\end{bmatrix}\hat{\mathbf{x}}=\begin{bmatrix}14\\57\end{bmatrix}
  \]
  After elimination we get $\hat{\mathbf{c}}=(59/26,17/26)$. Thus $C\approx2.27$ and $D\approx0.65$. The best fit line is roughly $2.27+0.65t=y$.

  We didn't really need the projection vector $\mathbf{p}$. In this case $\mathbf{p}\approx(3.58,4.23,6.19)$, which is simply the vector of $y$-coordinates along the best fit line. The error vector is $\mathbf{b}-\mathbf{p}\approx(-0.58,0.76,0.19)$ and corresponds to the ``residuals''.
\end{example}

For a larger data set $(t_i,b_i)$, the $A$ matrix and $\mathbf{b}$ vector will always look like the following:
\[A=\begin{bmatrix}
    1&t_1\\
    2&t_2\\
    \vdots&\vdots\\
    1&t_n
  \end{bmatrix}
  ,\quad
  \mathbf{b}=\begin{bmatrix}b_1\\b_2\\\vdots\\b_n\end{bmatrix}
\]

Here is the really cool part: The method can be used to find the best fit for any linear model, not just a line! Here a linear model means any linear combination of functions. For example we can find a best fit \emph{parabola} using the model $C+Dt+Et^2=y$.

\begin{example}
  Find the best fit parabola for the given data set.
  \[\begin{bmatrix}0\\6\end{bmatrix}
    \begin{bmatrix}1\\0\end{bmatrix}
    \begin{bmatrix}2\\0\end{bmatrix}
    \begin{bmatrix}3\\2\end{bmatrix}
  \]
  The data lie on a parabola if and only if the system is consistent:
  \[\systeme[CDE]{1C+0D+0E=6,1C+1D+1E=0,1C+2D+4E=0,1C+3D+9E=2}
  \]
  In other words, the $A$ matrix and $\mathbf{b}$ vector are:
  \[A=\begin{bmatrix}
      1&t_1&t_1^2\\
      1&t_2&t_2^2\\
      \vdots&\vdots&\vdots\\
      1&t_n&t_n^2
    \end{bmatrix}
    ,\quad
    \mathbf{b}=\begin{bmatrix}
      b_1\\b_2\\\vdots\\b_n
    \end{bmatrix}
  \]
  The best approximate solution turns out to be $\hat C=5.8,\hat D=-7.2,\hat E=2$. Thus the best fit parabola $5.8-7.2t+2t^2=y$. A graph of the data and the parabola appear below.
  
  \begin{center}
    \begin{tikzpicture}[scale=.7]
      \draw[gray!30,very thin,dashed] (-0.9,-0.9) grid (3.9,6.9);
      \draw[->,thick] (-1,0)--(4,0) node[right] {$t$};
      \draw[->,thick] (0,-1)--(0,7) node[left] {$y$};
      \draw[fill] (0,6) circle (.1);
      \draw[fill] (1,0) circle (.1);
      \draw[fill] (2,0) circle (.1);
      \draw[fill] (3,2) circle (.1);
      \draw[domain=-.1:3.1,dashed] plot (\x,5.8-7.2*\x+2*\x*\x) node[right]{???};
    \end{tikzpicture}
  \end{center}
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Use the method of orthogonal projections to find the best fit line for the data set. Then create an accurate plot of the data together with your proposed best fit line.
  \begin{enumerate}
    \item $(0,9),(1,4),(2,1)$
    \item $(0,0),(1,8),(3,8),(4,20)$
    \item $(-2,4),(-1,2),(0,-1),(1,0),(2,0)$
  \end{enumerate}
  \item Repeat problem 1(b), but this time find the best fit parabola. Then create an accurate plot of the data together with the best fit parabola.
  \item In problem 1(c), was there something special about the data that helped you find the answer more quickly? Explain.
\end{activity}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Determinants, eigenvalues, eigenvectors, and applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The determinant of a matrix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 5.1.

In this section we introduce the concept of the determinant of a matrix. Given any square matrix $A$, we will associate with it a determinant $\det(A)$, which is an important real number that tells us some information about $A$.

As motivation, consider the initial step in the process of inverting a general $2\times2$ matrix:
\begin{align*}
  \begin{bmatrix}a&b&\vline&1&0\\c&d&\vline&0&1\end{bmatrix}
  &\to\begin{bmatrix}a&b&\vline&1&0\\
    0&d-\frac{c}{a}b&\vline&-\frac{c}{a}&1\end{bmatrix}\\
  &\to\begin{bmatrix}a&b&\vline&1&0\\0&ad-bc&\vline&-c&a\end{bmatrix}\\
  &\to\begin{bmatrix}a&0&\vline&1+\frac{b}{ad-bc}c&-\frac{b}{ad-bc}a\\
    0&ad-bc&\vline&-c&a\end{bmatrix}\\
  &\to\begin{bmatrix}1&0&\vline&\frac{d}{ad-bc}&\frac{-b}{ad-bc}\\
    0&1&\vline&\frac{-c}{ad-bc}&\frac{a}{ad-bc}\end{bmatrix}\\
\end{align*}
Notice that after we cleared denominators, the last pivot is $ad-bc$, and that this quantity shows up in the denominator of the entries of $A^{-1}$. Notice also that if this quantity is zero, then the method fails and $A$ is not invertible.

The special quantity $ad-bc$ is called the \emph{determinant} of $A$ because it determines whether $A$ is invertible or not. For $A=$ the general $2\times2$ matrix, we write $\det(A)=ad-bc$ or $|A|=ad-bc$.

Of course we want to go beyond the simple case of a $2\times2$ matrix and find determinants for larger matrices. So let's step up to the case of a general $3\times3$ matrix. This time we won't bother calculating the inverse, but just the last pivot.
\begin{align*}
  \begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}
  &\to\begin{bmatrix}a&b&c\\0&e-\frac dab&f-\frac dac
    \\0&h-\frac gab&i-\frac gac\end{bmatrix}\\
  &\to\ldots\\
  &\to\begin{bmatrix}*&*&*\\0&*&*\\0&0&aei+bfg+cdh-afh-bdi-ceg\end{bmatrix}
\end{align*}
Thus if $A=$ the general $3\times3$ matrix, we write $\det(A)=aei+bfg+cdh-afh-bdi-ceg$.

These small cases are useful, but still leave us with a lot of questions about the determinant. What is it really, and what makes the determinant the determinant? We now proceed to define the determinant by its basic properties.

\begin{definition}
  The determinant $\det(A)$ is a mapping from matrices to real numbers and is defined by the following four properties.
  \begin{itemize}
    \item (Scaling) Scaling a row of by a scalar $t$ also scales the determinant by $t$.
    \[\det\begin{bmatrix}ta&tb\\c&d\end{bmatrix}
      =tad-tbc=t(ad-bc)
    \]
    \item (Row exchange) Exchanging two rows negates the determinant.
    \[\det\begin{bmatrix}c&d\\a&b\end{bmatrix}
      =cb-da=-(ad-bc)
    \]
    \item (Elimination) Adding a of one row to another row has no effect on the determinant.
    \[\det\begin{bmatrix}a+tc&b+td\\c&d\end{bmatrix}
      =ad-bc
    \]
    \item (Identity) The determinant of the identity matrix is $1$.
    \[\det\begin{bmatrix}1\\&1\end{bmatrix}=1
    \]
  \end{itemize}
\end{definition}

The above definition doesn't consist of a formula, but instead of several rules. These rules can be used to find the determinant of any matrix! To see how, we use the following reasoning.

\begin{itemize}
  \item The determinant of a diagonal matrix is the product of its diagonal entries. To see this we use the Scaling rule repeatedly plus the Identity rule.
    \[\det\begin{bmatrix}d_1&&\\&d_2&\\&&\ddots\end{bmatrix}
      =d_1\det\begin{bmatrix}1&&\\&d_2&\\&&\ddots\end{bmatrix}
      =\cdots=d_1d_2\cdots d_n
    \]
  \item The determinant of an upper triangular matrix is the product of its diagonal entries too. To see this note that if the determinant is not zero, it has all its pivots and you can use the Elimination rule until it is diagonal. Then apply the previous item.
  \item The determinant of any matrix is the product of its pivots. Once again, we can use the Elimination rule to make any matrix upper triangular, and then apply the previous item.
\end{itemize}

In conclusion, we can always find the determinant by eliminating it to upper triangular form, and then multiplying the pivots.

Warning: if you use any Scaling or Row Exchange steps while eliminating, you must keep track of this along the way.

\begin{example}
  \[\det\begin{bmatrix}1&2&3\\3&0&1\\1&2&1\end{bmatrix}
  \]
\end{example}

\begin{example}
  \[\det\begin{bmatrix}2&4&6\\2&4&1\\1&4&2\end{bmatrix}
  \]
\end{example}
  
\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Find the determinant of each of the following matrices. (Use elimination to make the matrix upper triangular, then multiply the pivots.)
  \[\text{(a) }\begin{bmatrix}2&-1&0\\1&2&-2\\-1&1&3\end{bmatrix}\qquad
    \text{(b) }\begin{bmatrix}a&1&1\\0&a&1\\0&0&a\end{bmatrix}\qquad
  \]
  \[\text{(c) }\begin{bmatrix}a&1&&\\1&a&1&\\&1&a&1\\&&1&a\end{bmatrix}\qquad
    \text{(d) }\begin{bmatrix}a&b&&\\c&d&&\\&&a&b\\&&c&d\end{bmatrix}\qquad
  \]
  \item For each $2\times 2$ matrix $A$ below, calculate the determinant using $ad-bc$. Then find the inverse (if it exists) using the formula $A^{-1}=\frac{1}{\det(A)}\begin{bmatrix}d&-b\\-c&a\end{bmatrix}$. Finally check your work by verifying that $AA^{-1}=I$.
  \[\text{(a) }\begin{bmatrix}2&-1\\1&2\end{bmatrix}\qquad
    \text{(b) }\begin{bmatrix}\frac12&-3\\5&\frac13\end{bmatrix}\qquad
    \text{(c) }\begin{bmatrix}-.2&.8\\.3&.7\end{bmatrix}
  \]
  \item Take a look at the six-term formula for the determinant of the general $3\times3$ matrix
  \[\det(A)=aei+bfg+cdh-afh-bdi-ceg
  \]
  What do all six terms have in common? Every term has a factor from each \underline{\hspace{.5in}}\ and each \underline{\hspace{.5in}}. What do the three positive terms have in common? What do the three negative terms have in common?
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Formulas for the determinant}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 5.2.

Last week we defined the determinant by its four key properties, and described an algorithm to calculate the determinant using elimination. While elimination is the most efficient method to calculate the determinant, formulas are still valuable for theoretical purposes and some applications. In this section we give one complex formula and one recursive formula.

We begin with the general formula for the determinant, called the \emph{big formula}. To motivate it, recall the six-term formula for the determinant of a $3\times3$ matrix:
\[\det\begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}
  =aei+bfg+cdh-afh-bdi-ceg
\]
Observe that each term contains one entry from every row and one entry from every column:
\smaller\smaller\smaller
\[\det\begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}
  =\det\begin{bmatrix}a\\&e\\&&i\\\end{bmatrix}
  +\det\begin{bmatrix}&b\\&&f\\g\\\end{bmatrix}
  +\det\begin{bmatrix}&&c\\d\\&h\\\end{bmatrix}
  -\det\begin{bmatrix}a\\&&f\\&h\\\end{bmatrix}
  -\det\begin{bmatrix}&b\\d\\&&i\\\end{bmatrix}
  -\det\begin{bmatrix}&&c\\&e\\g\\\end{bmatrix}
\]
\normalsize
A selection of one entry from every row and column is called a \emph{permutaion}. A permutation is a bijective function $\sigma\colon n\to n$. The set of all permutations of $1,\ldots,n$ is denoted $S_n$. Each term furthermore has a \emph{sign}, which is $\pm1$ depending on whether it takes an even or odd number of row exchanges to make it diagonal.

In sum, the determinant of a matrix is given by the following ``big'' formula:
\[\det(A)=
  \sum_{\sigma\in S_n}\mathrm{sign}(\sigma)\prod_1^na_{i,\sigma(i)}
\]
The formula is ``big'' because it has $n!$ many terms, a huge quantity. For this reason it is of little practical value unless the matrix is sparse (mostly zeros). It is also valuable for some theoretical purposes.

\begin{example}
  Use the general formula to find the determinant of the matrix.
  \[\begin{bmatrix}&1&&\\1&&1\\&1&&1\\&&1\end{bmatrix}
  \]
\end{example}

We now introduce the recursive formula for the determinant, called the \emph{cofactor expansion}. We once again begin with the six term formula for the determinant of a $3\times3$ matrix. This time we factor some of the terms:
\[\det\begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}
  =a(ei-fh)+b(fg-di)+c(dh-eg)
\]
The last equation is a combination of three sub-determinants or ``cofactors'':
\[\det\begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}
  =a\det\begin{bmatrix}e&f\\h&i\end{bmatrix}
  -b\det\begin{bmatrix}d&f\\g&i\end{bmatrix}
  +c\det\begin{bmatrix}d&e\\g&h\end{bmatrix}
\]

More generally, given a matrix $A$ consisting of entries $a_{ij}$, we define the \emph{cofactor} $c_{ij}$ to be $(-1)^{i+j}$ times the determinant of the matrix obtained from $A$ by deleting the $i$th row and $j$th column. For example in the matrix
\[A=\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}
\]
the cofactor of the $4$ is $-\det\begin{bmatrix}2&3\\8&9\end{bmatrix}=6$.

The \emph{cofactor formula} for the determinant is:
\[\det(A)=a_{11}c_{11}+\cdots+a_{1n}c_{1n}
\]

To remember the signs of the cofactors $(-1)^{i+j}$ we use the following memory device:
\[\begin{bmatrix}
    +&-&+\\-&+&+&\cdots\\+&-&+\\&\vdots&&\ddots
  \end{bmatrix}
\]

\begin{example}
  Use the cofactor expansion to find the determinant of the matrix.
  \[\det\begin{bmatrix}
    2&-1\\-1&2&-1\\&-1&2&-1\\&&-1&2
  \end{bmatrix}
  \]
\end{example}

We close by noting that you don't necessarily have to use the top row to make a cofactor expansion. You can use any row or column, provided you are careful to get the signs of the cofactors correct.

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item For each of the following matrices, determine all ways of choosing one nonzero entry from each row and column. For each way you found, find the ``sign'' of that way. Finally use the ``big formula'' to find the determinant.
  \begin{enumerate}
    \item $\begin{bmatrix}1&-1\\&2&-1\\&&2&-1\\-1&&&2\end{bmatrix}$
    \item $\begin{bmatrix}1&&&2\\&3&4&5\\5&4&&3\\2&&&1\end{bmatrix}$
  \end{enumerate}
  \item 
  \begin{enumerate}
    \item Use the cofactor expansion along the first row to find the determinant of the matrix of 1(a).
    \item Use the cofactor expansion along the first row to find the determinant of the matrix of 1(b).
  \end{enumerate}
  \item If $A$ is a matrix, let $C$ be the matrix of cofactors of $A$. That is, $c_{ij}$ is the $ij$-cofactor of $A$. For example:
  \[A=\begin{bmatrix}1&2&3\\4&2&2\\1&1&1\end{bmatrix},\qquad
  C=\begin{bmatrix}0&-2&2\\1&-2&1\\-2&10&-6\end{bmatrix}
  \]

  \[A=\begin{bmatrix}3&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{bmatrix},\qquad
  C=\begin{bmatrix}1&0&0&0\\0&3&0&0\\0&0&3&0\\0&0&0&3\end{bmatrix}
  \]
  What is the relationship between the determinant of $A$ and the determinant of $C$? Test the $A,C$ pairs above and make a conjecture.
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two applications of determinants}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 5.3.

We have already said that the determinant of a matrix tells you whether it is invertible (nonzero determinant) or singular (zero determinant). We will use this fact repeatedly in the next sections. But before moving on, we give two further applications of the determinant.

The first application involves the close connection between determinants and the inverse matrix. The first hint of this was the general formula for the inverse of a $2\times 2$ matrix, in which $\det(A)$ appears in the denominator.
\[\begin{bmatrix}a&b\\c&d\end{bmatrix}^{-1}
  =\frac{1}{ad-bc}\begin{bmatrix}d&-b\\-c&a\end{bmatrix}
\]

For an even bigger hint, we will reveal the general formula for the inverse of a $3\times 3$ matrix.
\[\begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}^{-1}
  =\frac{1}{\det(A)}\begin{bmatrix}ei-fh&ch-bi&bf-ce\\fg-di&ai-cg&cd-af\\
  dh-eg&bg-ah&ae-bd\end{bmatrix}
\]
Once again the determinant appears in the denominator. But note also that the entries of the matrix are determinants as well, in fact they are the cofactors of the entries of $A$.

\begin{definition}
  Given a square matrix $A$, the \emph{cofactor matrix} of $A$ is the matrix $C$ whose $ij$-entry is the cofactor $c_{ij}$ (we remind the reader that $c_{ij}$ is equal to $(-1)^{i+j}$ times the determinant of the matrix obtained from $A$ by crossing out row $i$ and column $j$.
\end{definition}

\begin{example}
  The matrix $A$ has cofactor matrix $C$:
  \[A=\begin{bmatrix}1&2&3\\4&2&2\\1&1&1\end{bmatrix},\qquad
    C=\begin{bmatrix}0&-2&2\\1&-2&1\\-2&10&-6\end{bmatrix}
  \]
\end{example}  

In the above example, look at what happens when we multiply $AC^T$:
\[AC^T=\begin{bmatrix}1&2&3\\4&2&2\\1&1&1\end{bmatrix}
  \begin{bmatrix}0&1&-2\\-2&-2&10\\2&1&-6\end{bmatrix}
  =\begin{bmatrix}2\\&2\\&&2\end{bmatrix}
\]
In general the product $AC^T$ is equal to $\det(A)I$, the diagonal matrix with the determinant of $A$ every diagonal entry.
\[\begin{bmatrix}
    a_{11}&\cdots&a_{1n}\\
    \vdots&&\vdots\\
    a_{n1}&\cdots&a_{nn}
  \end{bmatrix}
  \begin{bmatrix}
    c_{11}&\cdots&c_{1n}\\
    \vdots&&\vdots\\
    c_{n1}&\cdots&c_{nn}
  \end{bmatrix}
  =\begin{bmatrix}
    \det(A)\\&\ddots\\&&\det(A)
  \end{bmatrix}
\]
% While this may seem mysterious at first, this really just comes from the cofactor expansion of the determinant. Every diagonal entry of $AC^T$ is of the form $a_{i1}c_{i1}+\cdots+a_{in}c_{in}$ and this equals $\det(A)$. Note that the off-diagonal entries are all zero because they correspond to the cofactor expansion of the determinant of a matrix with a duplicate row.

\begin{theorem}[Cofactor for the inverse]
  \[A^{-1}=\frac{1}{\det(A)}C^T
  \]
\end{theorem}

\begin{example}
  \[A=\begin{bmatrix}1\\1&1\\1&1&1\\1&1&1&1\end{bmatrix},\quad
    A^{-1}=\begin{bmatrix}1\\-1&1\\&-1&1\\&&-1&1\end{bmatrix}
  \]
\end{example}

The second application is about the calculation of the area of a parallelepiped. Recall that a parallelepiped is the higher dimensional analog of a paralellogram and is the region bounded by several vectors and their sums.

Let's first look at the $2$-dimensional parallelogram with vector sides $(a,b)$ and $(c,d)$. Using simple geometry, we can find that the area of the paralellogram is $ad-bc$, the determinant of the matrix consisting of the two vector sides.
\begin{center}
  \begin{tikzpicture}[yscale=.7]
    \draw[->] (-.5,0)--(5.5,0);
    \draw[->] (0,-.5)--(0,3.5);
    \draw (0,0) rectangle (5,3);
    \draw (0,0) -- (1,2) -- (5,3) -- (4,1) -- (0,0);
    \draw (4,0) -- (4,1) -- (5,1);
    \draw (0,2) -- (1,2) -- (1,3);
    \node[below] at (2,0) {$a$};
    \node[right] at (5,.5) {$b$};
    \node[below] at (4.5,0) {$c$};
    \node[right] at (5,2) {$d$};
  \end{tikzpicture}
\end{center}

Perhaps surprisingly, this turns out to be true in higher dimensions! The $n$-dimensional volume of the paralellepiped with vector edges $\mathbf{v}_1,\ldots,\mathbf{v}_n$ is given by
\[V=\left|\det\begin{bmatrix}
      |&&|\\
      \mathbf{v}_1&\cdots&\mathbf{v}_n\\
      |&&|
    \end{bmatrix}\right|
\]

To understand briefly how this can be true, we can see that the volume of a parallelepiped satisfies the same four properties that we used to define the determinant.
\begin{itemize}
  \item (Identity) If $\mathbf{v}_i=\mathbf{e}_i$ then the parallelepiped is the unit hypercube which clearly has volume $1$.
  \item (Scaling) If you scale one of the sides of a paralellepiped by $t$ this clearly scales the volume by $t$.
  \item (Elimination) Adding a multiple of one vector to another slides the parallelepiped along one of its sides. This has no effect on the volume.
  \item (Row exchange) This has no effect on the volume, but if we consider the volume to be signed, we can assume this is true.
\end{itemize}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Use \textbf{geometry} to find the area of the parallelogram with corners $(0,0)$, $(4,1)$, $(1,2)$, $(5,3)$. Then use a \textbf{determinant} to confirm your answer.
  \item Find the volume of the parallelepiped with the vector sides.
    \[\begin{bmatrix}1\\1\\1\end{bmatrix},
      \begin{bmatrix}1\\2\\3\end{bmatrix},
      \begin{bmatrix}1\\3\\4\end{bmatrix}
    \]
  \item Use the cofactor formula for the inverse to find the inverse of each matrix (if it exists).
  \begin{enumerate}
    \item $\begin{bmatrix}0&2&-1\\2&1&5\\-1&0&4\end{bmatrix}$
    \item $\begin{bmatrix}a&&\\b&c&\\d&e&f\end{bmatrix}$
  \end{enumerate}
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Eigenvalues and eigenvectors}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 6.1.

We are now prepared to introduce eigenvalues and eigenvectors of a matrix. This is probably the most powerful and broadly used concept in linear algebra. Before presenting the technical definitions, let's begin with a motivating example.

Imagine a large island with two cities, $C$ and $D$. The two cities start with equal population of $50$ thousand. Every year 20\% of the population of city $C$ moves to city $D$, and 30\% of city $D$ moves to city $C$.
\begin{center}
  \begin{tikzpicture}
    \node at (0,0) (A) {$C$};
    \node at (2,0) (B) {$D$};
    \draw[->,bend right] (A) edge node[below]{20\%} (B);
    \draw[->,bend right] (B) edge node[above]{30\%} (A);
  \end{tikzpicture}
\end{center}
What will happen to the two populations over time? It isn't easy to tell just by guessing! We can look at the first few years:
\begin{center}
  \begin{tabular}{ccccc}
     & 0yr & 1yr & 2yr & 3yr \\\hline
    $C$ & 50 & 55 & 57.5 & 58.75 \\
    $D$ & 50 & 45 & 42.5 & 41.25
  \end{tabular}
\end{center}
It looks like in the limit, the population distribution will tend towards 60\% for city $C$ and 40\% for city $D$. How could we find the limit population distribution without just estimating? Linear algebra can come to our aid. It turns out that the dynamics of this scenario are controled by a matrix, and the limit population distribution is an \emph{eigenvector} associated with that matrix.

\begin{definition}
  Let $A$ be a square matrix. Let $\mathbf{x}$ be a vector and $\lambda$ be a number such that $A\mathbf{x}=\lambda\mathbf{x}$. Then the vector $\mathbf x$ is called an \emph{eigenvector} of $A$ and the number $\lambda$ is called an \emph{eigenvalue} of $A$.
\end{definition}

If $\mathbf{x}$ is an eigenvector of $A$, then $\mathbf{x}$ is special because $A$ doesn't change what direction it points in. It simply scales $\mathbf{x}$ by a factor of $\lambda$.

We now turn to the problem of finding eigenvectors and eigenvalues for a given matrix $A$. First we will lay out the theory, and later we will show how it works with an example. Given a matrix $A$, we follow the following steps.

\begin{itemize}
  \item Step 0. We are looking for $\mathbf{x}$, $\lambda$ which make $A\mathbf{x}=\lambda\mathbf{x}$ true. We rewrite this as $A\mathbf{x}-\lambda\mathbf{x}=0$, and then again as $(A-\lambda I)\mathbf{x}=\mathbf{0}$.
  \item Step 1. The last equation tells us we want to find out when $A-\lambda I$ has a nontrivial null space. We know this happens when $A-\lambda I$ is singular, or in other words when $\det(A-\lambda I)=0$. So we set $\det(A-\lambda I)=0$ and solve for $\lambda$.
  \item Step 2. For each solution $\lambda$ obtained in step $1$, plug it into $A-\lambda I$ and find the basis for the null space. This vector is the eigenvector $\mathbf{x}$ corresponding to $\lambda$. (Sometimes there will be several vectors $\mathbf{x}$ corresponding to $\lambda$.)
\end{itemize}

\begin{example}
  Consider the matrix $A$:
  \[A=\begin{bmatrix}1&2\\2&4\end{bmatrix}
  \]
  Beginning with Step 1, we have
  \begin{align*}
    \det\begin{bmatrix}1-\lambda&2\\2&4-\lambda\end{bmatrix}=0
    &\iff (1-\lambda)(4-\lambda)-(2)(2)=0\\
    &\iff 4-5\lambda+\lambda^2-4=0\\
    &\iff \lambda^2-5\lambda=0\\
    &\iff (\lambda)(\lambda-5)=0
  \end{align*}
  Thus the eigenvalues are $0$ and $5$. To find the eigenvectors we move on to Step 2. For the eigenvalue $\lambda=0$ we find the null space of
  \[\begin{bmatrix}1-0&2\\2&4-0\end{bmatrix}
  \]
  Thus the eigenvector corresponding to $\lambda=0$ is $\mathbf{x}=(-2,1)^T$. For the eigenvalue $\lambda=5$ we find the null space of 
  \[\begin{bmatrix}1-5&2\\2&4-5\end{bmatrix}
  \]
  Thus the eigenvector corresponding to $\lambda=5$ is $\mathbf{x}=(1,2)^T$. We can check our solutions by confirming that $A(-2,1)^T=0(-2,1)^T$ and $A(1,2)^T=5(1,2)^T$.
\end{example}
  
\begin{example}
  Here we return to our motivating example with cities $C$ and $D$. To go from the population distribution for year $n$ to the population distribution for year $n+1$, we multiply by the matrix $A$:
  \[A=\begin{bmatrix}0.8&0.3\\0.2&0.7\end{bmatrix}
  \]
  Now we will find the eigenvalues and eigenvectors of $A$. The equation in Step 1 will be:
  \[(0.8-\lambda)(0.7-\lambda)-(0.2)(0.3)=0
  \]
  The solutions are $\lambda=1,0$. The corresponding eigenvectors are $(1.5,1)^T$ and $(1,-1)^T$. The eigenvector corresponding to $\lambda=1$ is the one we are after (we will see why this is true later). Note that if we renormalize $(1.5,1)^T$ by dividing by the sum $2.5$, it becomes $(0.6,0.4)^T$, which is the limit population distribution!
\end{example}

\begin{example}
  Consider the matrix $A$:
  \[A=\begin{bmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{bmatrix}
  \]
  We find that it has eigenvalues $0,1,3$ with corresponding eigenvectors $(1,1,1)$, $(-1,0,1)$, $(1,-2,1)$. Check!
\end{example}

% Do a complex example??? At least show the complex eigenvalues, but don't necessarily continue to the eigenvectors.

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item For each matrix $A$, find the eigenvalues $\lambda$ and corresponding eigenvectors $\mathbf{x}$. In each case, check that $A\mathbf{x}=\lambda\mathbf{x}$.
  \begin{enumerate}
    \item $A=\begin{bmatrix}0&2\\1&1\end{bmatrix}$
    \item $A=\begin{bmatrix}
      3 & 4 & 2 \\
      0 & 1 & 2 \\
      0 & 0 & 0
      \end{bmatrix}$
    \item $A=\begin{bmatrix}
      0 & 2 & 0 & 0 \\
      2 & 0 & 0 & 0 \\
      0 & 0 & 0 & 2 \\
      0 & 0 & 2 & 0
    \end{bmatrix}$
  \end{enumerate}
  \item Suppose $A$ and $B$ are cities, and that each year 10\% of city $A$ movies to city $B$, and 20\% of city $B$ movies to city $A$. Assuming there are no other population changes, find the matrix $A$ which models the population distribution over time. Then find the eigenvalues and eigenvectors of $A$. What is the limit population distribution?
  \item Let $A$ be the matrix from problem 1(b). Put the three eigenvectors you found into the columns of a new matrix $X$. How is $AX$ related to $X$?
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Diagonalization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 6.2.

In the previous section we said that for a square matrix $A$, if the special equation $A\mathbf{x}=\lambda\mathbf{x}$ is true then we say that $\lambda$ is an eigenvalue and $\mathbf{x}$ is a corresponding eigenvector of $A$. In most cases, if $A$ is $n\times n$ then there will be exactly $n$ distinct eigenvectors (not counting scalar multiples). Moreover the eigenvectors are linearly independent, which means they make a \emph{basis} for $\R^n$. The basis of eigenvectors is very special, because $A$ acts really nicely (diagonally) with respect to this basis.

To begin exploring this idea, assume that $A$ is $n\times n$ and has $n$ distinct eigenvectors. We let $X$ be the $n\times n$ matrix whose columns are the eigenvectors. Then the matrices $A$ and $X$ interact in a very special way, which we will explore by example first.

\begin{example}
  Let $A$ be the matrix:
  \[A=\begin{bmatrix}
      3 & 4 & 2 \\
      0 & 1 & 2 \\
      0 & 0 & 0
      \end{bmatrix}
  \]
  The eigenvalues are $\lambda=3,1,0$, and the corresponding eigenvectors are $(1,0,0)^T$, $(-2,1,0)^T$, and $(2,-2,1)^T$. When we multiply $AX$ we get:
  \[AX=\begin{bmatrix}
      3 & 4 & 2 \\
      0 & 1 & 2 \\
      0 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & -2 & 2 \\
      0 & 1 & -2 \\
      0 & 0 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
      3 & -2 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0
    \end{bmatrix}      
  \]
\end{example}

We find that $AX=$ the matrix $X$ again but with each column multiplied by the corresponding eigenvalue. We can write this principle in matrix form as follows.

\[AX=A\begin{bmatrix}
    \vdots&&\vdots\\
    \mathbf{x}_1&\cdots&\mathbf{x}_n\\
    \vdots&&\vdots
  \end{bmatrix}
  =\begin{bmatrix}
    \vdots&&\vdots\\
    \lambda_1\mathbf{x}_1&\cdots&\lambda_n\mathbf{x}_n\\
    \vdots&&\vdots
  \end{bmatrix}
  =\begin{bmatrix}
    \vdots&&\vdots\\
    \mathbf{x}_1&\cdots&\mathbf{x}_n\\
    \vdots&&\vdots
  \end{bmatrix}
  \begin{bmatrix}
    \lambda_1\\&\ddots&\\&&\lambda_n
  \end{bmatrix}
\]

Let $\Lambda$ be the diagonal matrix with eigenvalues in the diagonal entries. We can summarize the above by saying that $AX=X\Lambda$. We can further rewrite this as
\[A=X\Lambda X^{-1}
\]
This equation is said to \emph{diagonalize} the matrix $A$. Of course most matrices $A$ are not diagonal, but this equation shows that in most cases it is possible to transform $A$ to make it diagonal!

\begin{example}
  Let $A$ be the matrix below. Diagonalize $A$ by writing it as $A=X\Lambda X^{-1}$.
  \[A=\begin{bmatrix}1&5\\&6\end{bmatrix}
  \]
  The eigenvalues are $\lambda=1,6$ and corresponding eigenvectors are $(1,0)^T$ and $(1,-1)^T$. We use these to populate the matrices $\Lambda$ and $X$. We also have to find the inverse of $X$. The solution is:
  \[A=\begin{bmatrix}1&1\\&1\end{bmatrix}
    \begin{bmatrix}1&\\&6\end{bmatrix}
    \begin{bmatrix}1&-1\\&1\end{bmatrix}
  \]
  We can check our answer by confirming that the right-hand side multiplies back to $A$.
\end{example}

There are many applications of diagonalization. One application is the calculation of matrix powers $A^k$. If you can diagonalize $A$ as $A=S\Lambda S^{-1}$, then you can also diagonalize its powers as $A^k=S\Lambda^kS^{-1}$. This is because the extra $S$'s cancel as follows:

\[A^k=S\Lambda S^{-1}S\Lambda S^{-1}\cdots S\Lambda S^{-1}
=S\Lambda\Lambda\cdots\Lambda S^{-1}=S\Lambda^kS^{-1}
\]

This application will come in handy when we study asymptotic behavior of linear models, such as the city population problem from the previous section.

\begin{example}
  For $A$ in the previous example, find $A^{100}$. The answer is
  \[A^{100}=\begin{bmatrix}1&1\\&1\end{bmatrix}
    \begin{bmatrix}1&\\&6\end{bmatrix}^{100}
    \begin{bmatrix}1&-1\\&1\end{bmatrix}
    =\begin{bmatrix}1&-1+6^{100}\\&6^{100}\end{bmatrix}
  \]
\end{example}

We conclude this section with one of the coolest applications of diagonalization. Recall that the Fibonacci sequence begins with $F_0=0$, $F_1=1$ and is defined recursively by $F_{n+1}=F_{n+1}+F_n$. Find an absolute formula for $F_n$.
\begin{itemize}
  \item Step 1. Using the variables $F_{n+1}$ and $F_n$, the process is governed by the equations
  \[\systeme{F_{n+1}=F_n+F_{n-1},F_n=F_n}
  \]
  Or in matrix terms we have
  \[\begin{bmatrix}F_{n+1}\\F_n\end{bmatrix}
    =\begin{bmatrix}1&1\\1&0\end{bmatrix}
    \begin{bmatrix}F_n\\F_{n-1}\end{bmatrix}
  \]
  Denoting this square matrix by $A$, we want to find
  \[\begin{bmatrix}F_{n+1}\\F_n\end{bmatrix}
    =A^n\begin{bmatrix}1\\0\end{bmatrix}
  \]
  \item Step 2. We diagonalize the matrix $A$ above. We find that the two eigenvalues are $\lambda_1=(1+\sqrt{5})/2$ and $\lambda_2=(1-\sqrt{5})/2$. The corresponding eigenvectors are $(\lambda_1,1)^T$ and $(\lambda_2,1)^T$. Thus we can diagonalize $A$ as
  \[A=\begin{bmatrix}\lambda_1&\lambda_2\\1&1\end{bmatrix}
    \begin{bmatrix}\lambda_1\\&\lambda_2\end{bmatrix}
    \frac{1}{\sqrt{5}}
    \begin{bmatrix}1&-\lambda_2\\-1&\lambda_1\end{bmatrix}
    \]
  \item Step 3. We are really interested in
  \begin{align*}
    \begin{bmatrix}F_{n+1}\\F_n\end{bmatrix}
    &=A^n\begin{bmatrix}1\\0\end{bmatrix}\\
    &=\begin{bmatrix}\lambda_1&\lambda_2\\1&1\end{bmatrix}
      \begin{bmatrix}\lambda_1\\&\lambda_2\end{bmatrix}^n
      \frac{1}{\sqrt{5}}
      \begin{bmatrix}1&-\lambda_2\\
        -1&\lambda_1\end{bmatrix}
      \begin{bmatrix}1\\0\end{bmatrix}\\
    &=\frac{1}{\sqrt{5}}
      \begin{bmatrix}\lambda_1&
        \lambda_2\\1&1\end{bmatrix}
      \begin{bmatrix}(\lambda_1)^n\\
        &(\lambda_2)^n\end{bmatrix}
      \begin{bmatrix}1&-\lambda_2\\
        -1&\lambda_1\end{bmatrix}
      \begin{bmatrix}1\\0\end{bmatrix}\\
    &=\frac{1}{\sqrt{5}}
      \begin{bmatrix}
        (\lambda_1)^{n+1}-(\lambda_2)^{n+1}\\
        (\lambda_1)^n-(\lambda_2)^n
      \end{bmatrix}
  \end{align*}
  Looking at just the second component of the last line, we conclude the final formula:
  \[F_n=\frac{1}{\sqrt{5}}
    \left(\left(\frac{1+\sqrt{5}}{2}\right)^n
    -\left(\frac{1-\sqrt{5}}{2}\right)^n\right)
  \]
\end{itemize}

% Define diagonalizable???

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item In the Fibonacci numbers we said $F_{n+1}$ was the sum of the previous two numbers. To construct the \emph{Gibonacci} numbers, we let $G_{n+1}=$ the \emph{average} of the previous two numbers.
  \begin{enumerate}
    \item Using the initial conditions $G_0=0$ and $G_1=1$, write the first $7$ terms of the sequence $G_n$.
    \item Write the system of equations that models the Gibonacci process.
    
      $\systeme{G_{n+1}=,G_n=}$
    \item Write the $2\times2$ matrix $A$ that models the Gibonacci process.
    \item Diagonalize the matrix $A$.
    \item Use your diagonalization to find a formula for the power $A^n$.
    \item Find the formula for $G_n$.
    \item Use the above information to find $\lim_{n\to\infty}G_n$.
  \end{enumerate}
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 10.3.

We introduced eigenvectors with a problem about population flow between cities. We observed that the limit population distribution seems to be given by the eigenvector corresponding to $\lambda=1$. Now that we have the diagonalization avialable, we can actually prove this is true.

As a reminder, the setup is that each year 20\% of the population of city $C$ moves to city $D$, and 30\% of city $D$ moves to city $C$. We said that the population flow is controlled by the matrix
\[A=\begin{bmatrix}.8&.3\\.2&.7\end{bmatrix}
\]
Thus the population distribution in the $n$th year is given by
\[\begin{bmatrix}C_n\\D_n\end{bmatrix}
  =A^n\begin{bmatrix}C_0\\D_0\end{bmatrix}
\]
To find this we will need to diagonalize $A$. We already found the eigenvalues are $\lambda=1,.5$ and the corresponding eigenvectors are $(1.5,1)^T$ and $(-1,1)^T$. Thus we can calculate:
\begin{align*}
  A^n\begin{bmatrix}C_0\\D_0\end{bmatrix}
  &=\begin{bmatrix}1.5&-1\\1&1\end{bmatrix}
    \begin{bmatrix}1^n\\&0.5^n\end{bmatrix}
    \frac{1}{2.5}
    \begin{bmatrix}1&1\\-1&1.5\end{bmatrix}
    \begin{bmatrix}C_0\\D_0\end{bmatrix}
\end{align*}
Taking a limit we botain
\begin{align*}
  A^\infty\begin{bmatrix}C_0\\D_0\end{bmatrix}
  &=\begin{bmatrix}1.5&-1\\1&1\end{bmatrix}
    \begin{bmatrix}1\\&0\end{bmatrix}
    \frac{1}{2.5}
    \begin{bmatrix}1&1\\-1&1.5\end{bmatrix}
    \begin{bmatrix}C_0\\D_0\end{bmatrix}\\
  &=\frac{1}{2.5}\begin{bmatrix}1.5&1.5\\1&1\end{bmatrix}
     \begin{bmatrix}C_0\\D_0\end{bmatrix}\\
  &=\frac{1}{2.5}\begin{bmatrix}1.5\\1\end{bmatrix}
   (C_0+D_0)
\end{align*}

This calculation in this example works for a very general class of problems. A vector that sums to $1$ is called a \emph{probability vector}. A matrix whose columns are all probability vectors is called a \emph{Markov matrix}.

\begin{theorem}[Markov matrix thoerem]
  Suppose $A$ is a Markov matrix whose entries are nonzero and $\mathbf{x_0}$ is a probability vector. Then $\lim_{n\to\infty}A^n\mathbf{x_0}$ is equal to the eigenvector corresponding to $\lambda=1$ (normalized to be a probability vector).
\end{theorem}

This theorem is actually a special case of the much more powerful Perron--Frobenius theorem: If $A$ is a (reasonable) matrix, then its dominant (greatest) eigenvalue $\lambda_m$ is real and positive. Then the growth rate of the powers $A^n\mathbf{x}$ is given by $\lambda_m$ and the limit distribution is given by the normalized eigenvector $\mathbf{x}_m$ corresponding to $\lambda_m$.

As an example of this theorem, we present the Leslie matrix, which describes how a population changes through the generations.
  
\begin{example}
  Suppose you observe the fertility rates and survival rates of three generations: Group $1$ consists of ages $0$ to $20$, Group $2$ of ages $20$ to $40$, and Group $3$ of ages $40$ to $60$. The fertility rates $F_n$ are the number of children each female in Group $n$ has in $20$ years, the survival rates $S_n$ are the proportion of Group $n$ that survives to group $n+1$.

  Suppose you measure $F_1=.04$, $F_2=1.1$, $F_3=.01$, $S_1=.98$, and $S_2=.92$. Then the process is modelled by the Leslie matrix:
  \[A=\begin{bmatrix}.04&1.1&.01\\.98\\&.92\end{bmatrix}
  \]
  This isn't a Markov matrix, but it is still subject to the Perron--Frobenius theorem. Using a calculator I found that the largest real eigenvalue was $\lambda=1.06$. Thus the population grows at a rate of 6\% per generation. The corresponding eigenvector, renormalized to sum to $1$, is $(.37,.34,.29)^T$. This tells the asymptotic distribution of the generations.
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Consider three towns with yearly population movements described by the figure. Assume there are no other population changes.
  \begin{center}
    \begin{tikzpicture}
      \node at (0:2.5) (C) {$C$};
      \node at (120:2.5) (D) {$D$}
      edge[->] node[below] {30\%} (C)
      edge[bend left,<-] node[above] {10\%} (C);
      \node at (240:2.5) (F) {$F$}
      edge[<-] node[above] {25\%} (C)
      edge[bend right,->] node[below] {20\%} (C)
      edge[->] node[right] {15\%} (D)
      edge[bend left,<-] node[left] {10\%} (D);
    \end{tikzpicture}
  \end{center}
  \begin{enumerate}
    \item Write the Markov matrix for these movements
    \item Find the eigenvector corresponding to $\lambda=1$.
    \item Find the limit population distribution of the three towns.
  \end{enumerate}
  \item  Suppose salmon have three equal growth stages: smolt, ocean dweller, and spawner. The fertility rate of smolts and ocean dwellers is 0, but the fertility rate of spawners is 12. The survival rate of smolts to ocean dwellers is $1/2$, and the survival rate of ocean dwellers to spawners is $1/3$.
  \begin{enumerate}
    \item Write the Leslie matrix for this model.
    \item Find is the dominant eigenvalue $\lambda_d$. What is the asymptotic growth rate of the salmon?
    \item Find the eigenvector corresponding to $\lambda_d$. What is the asymptotic distribution of the salmon?
  \end{enumerate}
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systems of differential equations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 6.3.

In our study of discrete processes such as Markov processes, we always have the recursive equation $\mathbf{x_{n+1}}=A\mathbf{x_n}$ and an initial state $\mathbf{x_0}$ given. Here the variables at time $n+1$ depend in a linear way on the variables at time $n$.

The continuous analog of this situation is a differential equation $\mathbf{u}'=A\mathbf{u}$ and an initial condition $\mathbf{u}(0)=\mathbf{u_0}$. Here the growth rate of the variables depends in a linear way on the variables themselves.

Beginning with the single variable case, suppose that $y'=ky$ and $y(0)=y_0$. This differential equation models unrestricted population growth, and we know from calculus that the solution is $y=y_0e^{kt}$, or exponential growth.

Moving to the multivariable version, suppose we are given the system of first-order linear ordinary differential equations:
\[\begin{cases}
    \mathbf{u}'=A\mathbf{u}\\
    \mathbf{u}(0)=\mathbf{u_0}
  \end{cases}
\]
It turns out that we can write the solution to such an equation as
\[\mathbf{u}(t)=e^{At}\mathbf{u_0}
\]
But this leaves us with three big questions. First, what exactly is $e^{At}$? Second, why does $e^{At}$ help solve the system of differential equations? And third, how can we calculate $e^{At}$?

For the first question (what is $e^{At}$), recall that $e^{kt}$ may be defined using the series $e^{kt}=1+kt+(kt)^2/2+(kt)^3/3!+\cdots$. We copy this idea and define:
\[e^{At}=I+At+\frac{(At)^2}{2}+\frac{(At)^3}{3!}+\cdots
\]
It is a fact that this infinite series of matrices always converges.

For the second question (why does $e^{At}\mathbf{u_0}$ solve $\mathbf{u}'=A\mathbf{u}$), we take the derivative of $e^{At}\mathbf{u_0}$ as follows:
\begin{align*}
  \frac{d}{dt}e^{At}\mathbf{u_0}
  &=\frac{d}{dt}\left[
    I+At+\frac{(At)^2}{2}+\frac{(At)^3}{3!}+\cdots
    \right]\mathbf{u_0}\\
  &=\left[0+A+A^2t+A\frac{(At)^2}{2}+\cdots\right]
    \mathbf{u_0}\\
  &=A\left[I+At+\frac{(At)^2}{2}+\cdots\right]\mathbf{u}_0\\
  &=Ae^{At}\mathbf{u_0}
\end{align*}
Thus we see that if $\mathbf{u}=e^{At}\mathbf{u_0}$, then $\mathbf{u}'=A\mathbf{u}$.

For the last question (how to calculate $e^{At}$), if $A$ is diagonalizable then one can compute $e^{At}$ using diagonalization. To see this, if $A=X\Lambda X^{-1}$ then
\begin{align*}
  e^{At}&=e^{X\Lambda t X^{-1}}\\
        &=I+X\Lambda X^{-1}t+\frac{X(\Lambda t)^2X^{-1}}{2}+\cdots\\
        &=X\left[I+\Lambda t+\frac{(\Lambda t)^2}{2}+\cdots\right]X^{-1}\\
        &=Xe^{\Lambda t}X^{-1}
\end{align*}
Since $\Lambda$ is diagonal, it is easy to calculate $e^{\Lambda t}$ simply by applying $e^{\lambda t}$ to each of its diagonal entries $\lambda$.

\begin{example}
  A gas diffuses between two adjacent chambers. Lte $u$ denotes the amount of gas in the first chamber and $v$ the amount in the second chamber, and assume the following equations are satisfied:
  \[\begin{cases}
      u'=-u+v & u(0)=15\\
      v'=u-v & v(0)=5
    \end{cases}
  \]
  
  To solve the system we use the matrix:
  \[A=\begin{bmatrix}-1&1\\1&-1\end{bmatrix}
  \]
  We can diagonalize $A$ as follows:
  \[A=\begin{bmatrix}1&1\\1&-1\end{bmatrix}
      \begin{bmatrix}0\\&-2\end{bmatrix}
      \frac{1}{-2}
      \begin{bmatrix}-1&-1\\-1&1\end{bmatrix}
  \]
  It follows that
  \[e^{At}=\begin{bmatrix}1&1\\1&-1\end{bmatrix}
    \begin{bmatrix}e^{0t}\\&e^{-2t}\end{bmatrix}
    \frac{1}{-2}
    \begin{bmatrix}-1&-1\\-1&1\end{bmatrix}
  \]
  And finally
  \[\mathbf{u}=e^{At}\mathbf{u}_0
    =\begin{bmatrix}1&1\\1&-1\end{bmatrix}
    \begin{bmatrix}1\\&e^{-2t}\end{bmatrix}
    \frac{1}{-2}
    \begin{bmatrix}-1&-1\\-1&1\end{bmatrix}
    \begin{bmatrix}15\\5\end{bmatrix}
  \]
  Carrying through the matrix products, we arrive at the solution:
  \[\mathbf{u}
    =\begin{bmatrix}10+5e^{-2t}\\10-5e^{-2t}\end{bmatrix}
  \]
  We observe that in the limit, the two chambers will have an equal proportion of gas.
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Consider the system of differential equations:
  \[\begin{cases}
      u'=u+v & u(0)=2\\
      v'=\quad\ 2v & v(0)=6
    \end{cases}
  \]
  \begin{enumerate}
    \item Let $A$ be the matrix below and find $e^{At}$.
    \[A=\begin{bmatrix}1&1\\0&2\end{bmatrix}
    \]
    \item Use the previuos part to solve the system of linear differential equations.
  \end{enumerate}
  \item The rabbit and wolf populations interact according to $r'=6r-2w$, and $w'=2r+w$. In other words, the rabbits breed very fast but lose some to wolf predation, and the wolves breed only modestly but better when there are rabbits.

  Suppose there are initially 30 rabbits and 30 wolves. Find formulas for the populations $r(t)$ and $w(t)$ of each species over time. After a long time, what is the ratio of rabbits to wolves?
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Search rankings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

ILA claims to have this material but doesn't :)

We conclude our introduction to eigenvalues and eigenvectors with one more application. This time we will look at the idea of ranking search results by their ``importance''.

When you perform an internet search, several things happen. First, the system filters just the sites that match to your search terms. Second, the system ranks the sites according to how important they are. One portion of this second part involves an algorithm that uses eigenvectors. (Google calls it PageRank.)

To see how it works, imagine a very small network with just five sites. We depict the sites as nodes and the links between them as arrows.
\begin{center}
  \begin{tikzpicture}[scale=1.2]
    \node at (0:1) (A) {$A$};
    \node at (72:1) (B) {$B$}
    edge[->] (A) edge[<-,bend left] (A);
    \node at (144:1) (C) {$C$} edge[<-] (A) edge[<-,bend left] (B);
    \node at (216:1) (D) {$D$} edge[<-] (A) edge[<-,bend left] (C);
    \node at (288:1) (E) {$E$} edge[<-] (C) edge[<-,bend left] (D);
  \end{tikzpicture}
\end{center}

We model browsing with a matrix as follows. We postulate a generic user who clicks links on the pages. Each time they open a page, they choose one of the links at random, with equal probability. If a page has no links, they choose one of the other pages at random. Thus for this internet we have
\[A=\begin{bmatrix}
    &1/2&&&1/5\\
    1/3&&&&1/5\\
    1/3&1/2&&&1/5\\
    1/3&&1/2&&1/5\\
    &&1/2&1&1/5
  \end{bmatrix}
\]

If we initially assume that the pages are visited with some probability vector $\mathbf{u}_0$, then $A^n\mathbf{u}_0$ gives the likelihood of being on each page after $n$ clicks. The big idea is that the limit $A^\infty\mathbf{u}_0$ gives us a reasonable measurement of the relative importance of each web page!

Since $A$ is a Markov matrix, we can calculate the limit simply by finding the eigenvector corresponding to $\lambda=1$. Using a computer, we find that $A-I$ has the RREF form
\[A-I\sim\begin{bmatrix}
    1&&&&-9/25\\
    &1&&&-8/25\\
    &&1&&-12/25\\
    &&&1&-14/25\\
    \phantom{1}
  \end{bmatrix}
\]
We conclude that the eigenvector is
\[(9/25,8/25,12/25,14/25,1)^T
\]
Which we can renormalize to
\[(0.13,0.12,0.18,0.21,0.37)^T
\]
This tells us that E is the most popular, followed by D.

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Consider the following (very very small) network.
  \begin{center}
    \begin{tikzpicture}[scale=1.2]
      \node at (0:1) (A) {$A$};
      \node at (72:1) (B) {$B$}  edge[<-] (A);
      \node at (144:1) (C) {$C$} edge[<-] (B);
      \node at (216:1) (D) {$D$} edge[<-] (B) edge[->] (C);
      \node at (288:1) (E) {$E$} edge[->] (B) edge[->] (A) edge[<-] (C);
    \end{tikzpicture}
  \end{center}
  \begin{enumerate}
    \item Write down a $5\times5$ matrix $A$ that represents this network.
    \item Find the eigenvector corresponding to $\lambda=1$ and rank the sites according to importance. (You may use a computer to help with the computation.)
  \end{enumerate}
  \item Create your own network with at least six sites. Try to make it unclear which site is most important. Repeat the steps above with your network to find out.
\end{activity}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Orthogonal bases and the SVD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Orthogonal bases}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 6.4.

We have previously discussed the concept of basis for $\R^n$: a family of $n$ vectors $\mathbf{v_1},\ldots,\mathbf{v_n}$ that are linearly independent. A basis is special because any vector in $\R^n$ can be written as a combination of basis vectors.

In this section we introduce the concept of \emph{orthogonal basis}: a family of $n$ vectors $\mathbf{v_1},\ldots,\mathbf{v_n}$ such that $\mathbf{v_i}\perp\mathbf{v_j}$ whenever $i\neq j$. An orthogonal basis is very special because it has the shape of the standard basis (the usual axes), except possibly rotated, reflected, and scaled.

An example of an orthogonal basis for $\R^2$ would be $(3,4)^T,(-4,3)^T$. Note that you can always normalize the vectors (make the norm $1$) to get what's called an \emph{orthonormal basis}. In this example just divide each vector by its length: $(3/5,4/5)^T,(-4/5,3/5)^T$.

An orthonormal basis is special algebraically. For example if $Q$ is a matrix  whose columns are an orthonormal basis, then $Q^{-1}=Q^T$.

\begin{example}
  Let $Q$ be the matrix
  \[Q=\frac16\begin{bmatrix}
    2\sqrt{3}&3\sqrt{2}&\sqrt{6}\\
    -2\sqrt{3}&0&2\sqrt{6}\\
    2\sqrt{3}&-3\sqrt{2}&\sqrt{6}
  \end{bmatrix}
  \]
  Then we can see that $Q^TQ=I$.
\end{example}

To see why this always works, note that the columns of $Q$ are the same as the rows of $Q^T$. When we take the dot product of a column with itself we get $1$ by normality, and when we take the dot product of a column with any other column we get $0$ by orthogonality.

Since we have seen the eigenvectors of a diagonalizable matrix always form a basis, it is natural to ask when the eigenvectors form an orthogonal basis.

\begin{example}
  Let $A$ be the matrix
  \[A=\begin{bmatrix}1&2\\2&4\end{bmatrix}
  \]
  Then $A$ has eigenvalues $\lambda=0,5$ and corresponding eigenvectors $(1,2)^T,(-2,1)^T$. These two eigenvectors are orthogonal.
\end{example}

The matrix in the previous example is \emph{symmetric}, meaning $A^T=A$. It turns out this always guarantees the eigenvectors are orthogonal, and more.

\begin{theorem}[Spectral theorem]
  Let $A$ be a real symmetric matrix. Then the eigenvalues of $A$ are all real, and the eigenvectors of $A$ are pairwise orthogonal. Thus $A$ can be diagonalized $A=Q\Lambda Q^T$, where the columns of $Q$ are orthonormal.
\end{theorem}

We will briefly explain why part of this theorem is true. First we show why a symmetric matrix will have real eigenvalues (recall it is fully possible for a matrix to have complex eigenvalues). Suppose $A$ is symmetric and that $A\mathbf{x}=\lambda\mathbf{x}$. Then we have:
\[\lambda\|\mathbf{x}\|=\lambda \bar{\mathbf{x}}^T\mathbf{x}
  =\bar{\mathbf{x}}^TA\mathbf{x}
  =\bar{\mathbf{x}}^TA^T\mathbf{x}
  =(\overline{A\mathbf{x}})^T\mathbf{x}
  =\bar\lambda\bar{\mathbf{x}}^T\mathbf{x}
  =\bar\lambda\|\mathbf{x}\|
\]
It follows that $\lambda=\bar\lambda$.

Next we show why a symmetric matrix will have orthogonal eigenvectors. Suppose $A$ is symmtric, $\lambda_1\neq\lambda_2$, and $A\mathbf{x_1}=\lambda_1\mathbf{x_1}$ and $A\mathbf{x_2}=\lambda_2\mathbf{x_2}$. Then we have:
\[\lambda_2\mathbf{x_1}^T\mathbf{x_2}
  =\mathbf{x_1}^TA\mathbf{x_2}
  =\mathbf{x_1}^TA^T\mathbf{x_2}
  =(A\mathbf{x_1})^T\mathbf{x_2}
  =\lambda_1\mathbf{x_1}^T\mathbf{x_2}
\]
It follows that $\mathbf{x_1}^T\mathbf{x_2}=\mathbf{0}$.

It remains only to prove that a symmetric matrix $A$ must always have exactly $n$ distinct eigenvectors. We leave this to the textbook.

\begin{example}
  Let $A$ be the matrix
  \[A=\begin{bmatrix}4&6\\6&9\end{bmatrix}
  \]
  Find the eigenvalues and eigenvectors, then diagonalize it $Q\Lambda Q^T$.
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item Find the eigenvalues and eigenvectors of the symmetric matrices. Check that the eigenvectors are orthogonal to each other. Diagonalize the matrix $A=Q\Lambda Q^T$.
  \begin{enumerate}
    \item $\begin{bmatrix}1&-1\\-1&1\end{bmatrix}$
    \item $\begin{bmatrix}1&3\\3&1\end{bmatrix}$
    \item $\begin{bmatrix}2&-1&0\\-1&2&-1\\0&-1&2\end{bmatrix}$
  \end{enumerate}
\end{activity}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The SVD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Based on ILA 5th, \S 7.2.

In the previous section we saw how to diagonalize a symmetric matrix $A=Q\Lambda Q^T$. Often in applications a matrix $A$ represents geometric data such as the pixels of an image. In this case $A$ will very large and its contents will be non-random. This usually means $A$ will have a few eigenvalues that are large (the signal) and many that are small (the noise). If that is true, we can create a compressed version of $A$ that is still a good approximation to $A$ by zeroing out the smaller eigenvalues (and their corresponding eigenvectors).

Of course if $A$ really does represent an image then it isn't very likely to be symmetric, or even square. In this section we generalize the method of diagonalization to arbitrary matrices. The cost is that the matrices on the left and right sides of $A$ won't be the same anymore. Instead we will have what is called the \emph{singular value decomposition} $A=U\Sigma V^T$.

The key idea to achieve this decomposition is that for any matrix $A$, the two matrices $A^TA$ and $AA^T$ are square and symmetric. The eigenvalues of $A^TA$ and $AA^T$ are the same, their square roots are called singular values, and they run down the diagonal of the $\Sigma$ in the SVD. The eigenvectors of $A^TA$ and $AA^T$ are both orthogonal bases, and they form the columns of $U$ and $V$ in the SVD.

Here are the steps to find the SVD of an $m\times n$ matrix $A$.

\begin{itemize}
  \item Find $A^TA$ and diagonalize it in an orthonormal way. Let $\lambda_i$ be the eigenvalues of $A^TA$ and $\mathbf{v_i}$ a corresponding orthonormal family of eigenvectors. The right singular vectors of $A$ are just the vectors $\mathbf{v_i}$. Put these into the columns of a matrix $V$.
  \item Next define the singular values of $A$ by $\sigma_i=\sqrt{\lambda_i}$. PUt these in the diagonal entries of an $m\times n$ matrix $\Sigma$.
  \item Now for each nonzero singular value $\sigma_i$ we define the left singular vector $\mathbf{u_i}=A\mathbf{v_i}/\sigma_i$. Put these in the columns of the matrix $U$.
  \item (We will avoid exercises that require this step, but if necessary, extend $U$ to an $m\times m$ matrix by adding additional columns orthogonal to the rest.)
\end{itemize}

Some explanation is necessary to see that these steps work. First to take $\sqrt{\lambda_i}$ we need to know $\lambda_i\geq0$. For this, observe that $\|A\mathbf{v_i}\|^2=\mathbf{v_i}^TA^TA\mathbf{v_i}=\mathbf{v_i}^T\lambda_i\mathbf{v_i}=\lambda_i\|\mathbf{v_i}\|^2=\lambda_i$.

Next, we know the $\mathbf{v_i}$ are orthogonal because they are eigenvectors of a symmetric matrix $A^TA$. We can additionally say that $\mathbf{u_i}$ are orthogonal because they are eigenvectors of $AA^T$. To see this, observe that $AA^T\mathbf{u_i}=AA^TA\mathbf{v_i}/\sigma_i=A\lambda_i\mathbf{v_i}/\sigma_i=\lambda_i\mathbf{u_i}$.

\begin{example}
  Let $A$ be the matrix below. Find the singular decomposition $A=U\Sigma V^T$.
  \[A=\begin{bmatrix}3&0\\4&5\end{bmatrix}
  \]
  To do this, we first find $A^TA$:
  \[A^TA=\begin{bmatrix}25&20\\20&25\end{bmatrix}
  \]
  As in the past, we can calculate that eigenvalues of $A^TA$ are $\lambda=45,5$ and the corresponding eigenvectors are $(1,1)^T$ and $(-1,1)^T$.
  
  Now the singular values are $\sigma=\sqrt{45},\sqrt{5}$, and the right singular vectors are $(1,1)^T/\sqrt{2}$ and $(-1,1)^T/\sqrt{2}$. Finally the left singular vectors are $(1,3)^T/\sqrt{10}$ and $(-3,1)^T/\sqrt{10}$. The final SVD is thus:
  \[A=\frac{1}{\sqrt{10}}\begin{bmatrix}1&-3\\3&1\end{bmatrix}
  \begin{bmatrix}\sqrt{45}\\&\sqrt{5}\end{bmatrix}
  \frac{1}{\sqrt{2}}\begin{bmatrix}1&1\\-1&1\end{bmatrix}
  \]
\end{example}

Note that in the above example, the eigenvalues of the original matrix are $5,3$. The singular decomposition has more information in its dominant singular value, and less information in its smaller singular value.

\begin{example}
  Let $A$ be the following nonsquare matrix. Find the singular decomposition $A=U\Sigma V^T$.
  \[A=\begin{bmatrix}0&1\\&&2\\&&&3\end{bmatrix}
  \]
  We first look at $A^TA$:
  \[A^TA=\begin{bmatrix}0\\&1\\&&4\\&&&9\end{bmatrix}
  \]
  We can see that the eigenvalues of $A^TA$ are $\lambda=9,4,1,0$ and the corresponding eigenvectors are $(0,0,0,1)^T$, $(0,0,1,0)^T$, $(0,1,0,0)^T$, and $(0,0,0,1)^T$.
  
  Now the singular values are $\sigma=3,2,1,0$ and the corresponding right singular vectors are just the same as above. To find the right singular vectors, for each $\sigma_i=3,2,1$ we calculate $A\mathbf{v_i}/\sigma$. We get the vectors $(0,0,1)^T$, $(0,1,0)^T$, and $(1,0,0)^T$. The final SVD is thus:
  \[A=\begin{bmatrix}&&1\\&1\\1&&\end{bmatrix}
  \begin{bmatrix}3\\&2\\&&1&0\end{bmatrix}
  \begin{bmatrix}&&&1\\&&1\\&1\\1\end{bmatrix}
  \]  
\end{example}

\newpage
\subsection*{Activity for \S \thesection}

\begin{activity}
  \item For each matrix $A$ find the singular value decomposition $A=U\Sigma V^T$.
  \begin{enumerate}
    \item $A=\begin{bmatrix}0&4\\0&0\end{bmatrix}$\\(this is one of those where you will need to put a second vector into the $U$ matrix, orthogonal to the first)
    \item $A=\begin{bmatrix}0&4\\1&0\end{bmatrix}$
    \item $A=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}$
  \end{enumerate}
\end{activity}


\end{document}
